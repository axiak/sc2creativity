{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [15, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import umap.plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>game_start</th>\n",
       "      <th>game_duration</th>\n",
       "      <th>self_won</th>\n",
       "      <th>self_name</th>\n",
       "      <th>self_race_is_protoss</th>\n",
       "      <th>self_race_is_zerg</th>\n",
       "      <th>self_race_is_terran</th>\n",
       "      <th>opponent_name</th>\n",
       "      <th>opponent_race_is_protoss</th>\n",
       "      <th>opponent_race_is_zerg</th>\n",
       "      <th>...</th>\n",
       "      <th>TwilightCouncil_start</th>\n",
       "      <th>TwilightCouncil_weight</th>\n",
       "      <th>VoidRay_start</th>\n",
       "      <th>VoidRay_weight</th>\n",
       "      <th>WarpGate_start</th>\n",
       "      <th>WarpGate_weight</th>\n",
       "      <th>WarpPrism_start</th>\n",
       "      <th>WarpPrism_weight</th>\n",
       "      <th>Zealot_start</th>\n",
       "      <th>Zealot_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20d6247127ed58eeca069051672d3e8c3598e132d2fc6c445040701499b72acf_0</th>\n",
       "      <td>2020-02-01 15:07:59</td>\n",
       "      <td>675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ShoWTimE</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Trap</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.176471</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.495627</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20d6247127ed58eeca069051672d3e8c3598e132d2fc6c445040701499b72acf_1</th>\n",
       "      <td>2020-02-01 15:07:59</td>\n",
       "      <td>675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Trap</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ShoWTimE</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.301775</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.470588</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50cb3b2f33692227e2e50e0e4fab529a44c4bfe9bfd9e8137c08a81e94f08370_1</th>\n",
       "      <td>2020-02-02 12:13:08</td>\n",
       "      <td>886</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ShoWTimE</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cure</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>18.750000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.930233</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.112360</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.454545</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e362432c504618e3a95055_0</th>\n",
       "      <td>2020-02-01 14:58:20</td>\n",
       "      <td>531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ShoWTimE</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Trap</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.302231</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.301775</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.610994</td>\n",
       "      <td>1.406126</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e362432c504618e3a95055_1</th>\n",
       "      <td>2020-02-01 14:58:20</td>\n",
       "      <td>531</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Trap</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ShoWTimE</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.470588</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.930233</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.809111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 113 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            game_start  \\\n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4... 2020-02-01 15:07:59   \n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4... 2020-02-01 15:07:59   \n",
       "50cb3b2f33692227e2e50e0e4fab529a44c4bfe9bfd9e81... 2020-02-02 12:13:08   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624... 2020-02-01 14:58:20   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624... 2020-02-01 14:58:20   \n",
       "\n",
       "                                                    game_duration  self_won  \\\n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...            675       1.0   \n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...            675       0.0   \n",
       "50cb3b2f33692227e2e50e0e4fab529a44c4bfe9bfd9e81...            886       0.0   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...            531       0.0   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...            531       1.0   \n",
       "\n",
       "                                                   self_name  \\\n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...  ShoWTimE   \n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...      Trap   \n",
       "50cb3b2f33692227e2e50e0e4fab529a44c4bfe9bfd9e81...  ShoWTimE   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...  ShoWTimE   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...      Trap   \n",
       "\n",
       "                                                    self_race_is_protoss  \\\n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...                   1.0   \n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...                   1.0   \n",
       "50cb3b2f33692227e2e50e0e4fab529a44c4bfe9bfd9e81...                   1.0   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...                   1.0   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...                   1.0   \n",
       "\n",
       "                                                    self_race_is_zerg  \\\n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...                0.0   \n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...                0.0   \n",
       "50cb3b2f33692227e2e50e0e4fab529a44c4bfe9bfd9e81...                0.0   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...                0.0   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...                0.0   \n",
       "\n",
       "                                                    self_race_is_terran  \\\n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...                  0.0   \n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...                  0.0   \n",
       "50cb3b2f33692227e2e50e0e4fab529a44c4bfe9bfd9e81...                  0.0   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...                  0.0   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...                  0.0   \n",
       "\n",
       "                                                   opponent_name  \\\n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...          Trap   \n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...      ShoWTimE   \n",
       "50cb3b2f33692227e2e50e0e4fab529a44c4bfe9bfd9e81...          cure   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...          Trap   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...      ShoWTimE   \n",
       "\n",
       "                                                    opponent_race_is_protoss  \\\n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...                       1.0   \n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...                       1.0   \n",
       "50cb3b2f33692227e2e50e0e4fab529a44c4bfe9bfd9e81...                       0.0   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...                       1.0   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...                       1.0   \n",
       "\n",
       "                                                    opponent_race_is_zerg  \\\n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...                    0.0   \n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...                    0.0   \n",
       "50cb3b2f33692227e2e50e0e4fab529a44c4bfe9bfd9e81...                    0.0   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...                    0.0   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...                    0.0   \n",
       "\n",
       "                                                    ...  \\\n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...  ...   \n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...  ...   \n",
       "50cb3b2f33692227e2e50e0e4fab529a44c4bfe9bfd9e81...  ...   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...  ...   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...  ...   \n",
       "\n",
       "                                                    TwilightCouncil_start  \\\n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...               1.000000   \n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...               1.000000   \n",
       "50cb3b2f33692227e2e50e0e4fab529a44c4bfe9bfd9e81...              18.750000   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...               7.302231   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...               8.470588   \n",
       "\n",
       "                                                    TwilightCouncil_weight  \\\n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...                     0.0   \n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...                     0.0   \n",
       "50cb3b2f33692227e2e50e0e4fab529a44c4bfe9bfd9e81...                     1.0   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...                     1.0   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...                     1.0   \n",
       "\n",
       "                                                    VoidRay_start  \\\n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...            1.0   \n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...            1.0   \n",
       "50cb3b2f33692227e2e50e0e4fab529a44c4bfe9bfd9e81...            1.0   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...            1.0   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...            1.0   \n",
       "\n",
       "                                                    VoidRay_weight  \\\n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...             0.0   \n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...             0.0   \n",
       "50cb3b2f33692227e2e50e0e4fab529a44c4bfe9bfd9e81...             0.0   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...             0.0   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...             0.0   \n",
       "\n",
       "                                                    WarpGate_start  \\\n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...       21.176471   \n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...       21.301775   \n",
       "50cb3b2f33692227e2e50e0e4fab529a44c4bfe9bfd9e81...       20.930233   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...       21.301775   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...       20.930233   \n",
       "\n",
       "                                                    WarpGate_weight  \\\n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...              1.0   \n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...              1.0   \n",
       "50cb3b2f33692227e2e50e0e4fab529a44c4bfe9bfd9e81...              1.0   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...              1.0   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...              1.0   \n",
       "\n",
       "                                                    WarpPrism_start  \\\n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...        10.495627   \n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...         8.470588   \n",
       "50cb3b2f33692227e2e50e0e4fab529a44c4bfe9bfd9e81...        10.112360   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...         7.610994   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...         7.809111   \n",
       "\n",
       "                                                    WarpPrism_weight  \\\n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...          1.000000   \n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...          1.000000   \n",
       "50cb3b2f33692227e2e50e0e4fab529a44c4bfe9bfd9e81...          1.000000   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...          1.406126   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...          1.000000   \n",
       "\n",
       "                                                    Zealot_start  \\\n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...      1.000000   \n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...      1.000000   \n",
       "50cb3b2f33692227e2e50e0e4fab529a44c4bfe9bfd9e81...     20.454545   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...      1.000000   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...      1.000000   \n",
       "\n",
       "                                                    Zealot_weight  \n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...            0.0  \n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...            0.0  \n",
       "50cb3b2f33692227e2e50e0e4fab529a44c4bfe9bfd9e81...            1.0  \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...            0.0  \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...            0.0  \n",
       "\n",
       "[5 rows x 113 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df = pd.read_hdf(\"../data/processed/summaries_protoss.hdf\", \"summaries\")\n",
    "raw_df = raw_df[raw_df.game_duration > 280]\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_columns = \"\"\"\n",
    "game_start\n",
    "game_duration\n",
    "self_name self_race_is_protoss self_race_is_zerg self_race_is_terran\n",
    "opponent_name\n",
    "\"\"\".split()\n",
    "data_columns = [col for col in raw_df.columns if col not in metadata_columns]\n",
    "\n",
    "df = raw_df[data_columns]\n",
    "\n",
    "encoding_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>self_won</th>\n",
       "      <th>opponent_race_is_protoss</th>\n",
       "      <th>opponent_race_is_zerg</th>\n",
       "      <th>opponent_race_is_terran</th>\n",
       "      <th>Adept_start</th>\n",
       "      <th>Adept_weight</th>\n",
       "      <th>AdeptPiercingAttack_start</th>\n",
       "      <th>AdeptPiercingAttack_weight</th>\n",
       "      <th>AirWeapons1_start</th>\n",
       "      <th>AirWeapons1_weight</th>\n",
       "      <th>...</th>\n",
       "      <th>TwilightCouncil_start</th>\n",
       "      <th>TwilightCouncil_weight</th>\n",
       "      <th>VoidRay_start</th>\n",
       "      <th>VoidRay_weight</th>\n",
       "      <th>WarpGate_start</th>\n",
       "      <th>WarpGate_weight</th>\n",
       "      <th>WarpPrism_start</th>\n",
       "      <th>WarpPrism_weight</th>\n",
       "      <th>Zealot_start</th>\n",
       "      <th>Zealot_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20d6247127ed58eeca069051672d3e8c3598e132d2fc6c445040701499b72acf_0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.176471</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.495627</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20d6247127ed58eeca069051672d3e8c3598e132d2fc6c445040701499b72acf_1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.495627</td>\n",
       "      <td>1.634720</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.301775</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.470588</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50cb3b2f33692227e2e50e0e4fab529a44c4bfe9bfd9e8137c08a81e94f08370_1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.052632</td>\n",
       "      <td>3.570540</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>18.750000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.930233</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.112360</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.454545</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e362432c504618e3a95055_0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.428571</td>\n",
       "      <td>1.784584</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.302231</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.301775</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.610994</td>\n",
       "      <td>1.406126</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e362432c504618e3a95055_1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.180124</td>\n",
       "      <td>1.390033</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.470588</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.930233</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.809111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    self_won  \\\n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...       1.0   \n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...       0.0   \n",
       "50cb3b2f33692227e2e50e0e4fab529a44c4bfe9bfd9e81...       0.0   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...       0.0   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...       1.0   \n",
       "\n",
       "                                                    opponent_race_is_protoss  \\\n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...                       1.0   \n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...                       1.0   \n",
       "50cb3b2f33692227e2e50e0e4fab529a44c4bfe9bfd9e81...                       0.0   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...                       1.0   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...                       1.0   \n",
       "\n",
       "                                                    opponent_race_is_zerg  \\\n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...                    0.0   \n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...                    0.0   \n",
       "50cb3b2f33692227e2e50e0e4fab529a44c4bfe9bfd9e81...                    0.0   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...                    0.0   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...                    0.0   \n",
       "\n",
       "                                                    opponent_race_is_terran  \\\n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...                      0.0   \n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...                      0.0   \n",
       "50cb3b2f33692227e2e50e0e4fab529a44c4bfe9bfd9e81...                      1.0   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...                      0.0   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...                      0.0   \n",
       "\n",
       "                                                    Adept_start  Adept_weight  \\\n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...     1.000000      0.000000   \n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...    10.495627      1.634720   \n",
       "50cb3b2f33692227e2e50e0e4fab529a44c4bfe9bfd9e81...    21.052632      3.570540   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...    21.428571      1.784584   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...    11.180124      1.390033   \n",
       "\n",
       "                                                    AdeptPiercingAttack_start  \\\n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...                        1.0   \n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...                        1.0   \n",
       "50cb3b2f33692227e2e50e0e4fab529a44c4bfe9bfd9e81...                        1.0   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...                        1.0   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...                        1.0   \n",
       "\n",
       "                                                    AdeptPiercingAttack_weight  \\\n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...                         0.0   \n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...                         0.0   \n",
       "50cb3b2f33692227e2e50e0e4fab529a44c4bfe9bfd9e81...                         0.0   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...                         0.0   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...                         0.0   \n",
       "\n",
       "                                                    AirWeapons1_start  \\\n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...                1.0   \n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...                1.0   \n",
       "50cb3b2f33692227e2e50e0e4fab529a44c4bfe9bfd9e81...                1.0   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...                1.0   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...                1.0   \n",
       "\n",
       "                                                    AirWeapons1_weight  ...  \\\n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...                 0.0  ...   \n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...                 0.0  ...   \n",
       "50cb3b2f33692227e2e50e0e4fab529a44c4bfe9bfd9e81...                 0.0  ...   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...                 0.0  ...   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...                 0.0  ...   \n",
       "\n",
       "                                                    TwilightCouncil_start  \\\n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...               1.000000   \n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...               1.000000   \n",
       "50cb3b2f33692227e2e50e0e4fab529a44c4bfe9bfd9e81...              18.750000   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...               7.302231   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...               8.470588   \n",
       "\n",
       "                                                    TwilightCouncil_weight  \\\n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...                     0.0   \n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...                     0.0   \n",
       "50cb3b2f33692227e2e50e0e4fab529a44c4bfe9bfd9e81...                     1.0   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...                     1.0   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...                     1.0   \n",
       "\n",
       "                                                    VoidRay_start  \\\n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...            1.0   \n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...            1.0   \n",
       "50cb3b2f33692227e2e50e0e4fab529a44c4bfe9bfd9e81...            1.0   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...            1.0   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...            1.0   \n",
       "\n",
       "                                                    VoidRay_weight  \\\n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...             0.0   \n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...             0.0   \n",
       "50cb3b2f33692227e2e50e0e4fab529a44c4bfe9bfd9e81...             0.0   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...             0.0   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...             0.0   \n",
       "\n",
       "                                                    WarpGate_start  \\\n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...       21.176471   \n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...       21.301775   \n",
       "50cb3b2f33692227e2e50e0e4fab529a44c4bfe9bfd9e81...       20.930233   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...       21.301775   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...       20.930233   \n",
       "\n",
       "                                                    WarpGate_weight  \\\n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...              1.0   \n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...              1.0   \n",
       "50cb3b2f33692227e2e50e0e4fab529a44c4bfe9bfd9e81...              1.0   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...              1.0   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...              1.0   \n",
       "\n",
       "                                                    WarpPrism_start  \\\n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...        10.495627   \n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...         8.470588   \n",
       "50cb3b2f33692227e2e50e0e4fab529a44c4bfe9bfd9e81...        10.112360   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...         7.610994   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...         7.809111   \n",
       "\n",
       "                                                    WarpPrism_weight  \\\n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...          1.000000   \n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...          1.000000   \n",
       "50cb3b2f33692227e2e50e0e4fab529a44c4bfe9bfd9e81...          1.000000   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...          1.406126   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...          1.000000   \n",
       "\n",
       "                                                    Zealot_start  \\\n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...      1.000000   \n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...      1.000000   \n",
       "50cb3b2f33692227e2e50e0e4fab529a44c4bfe9bfd9e81...     20.454545   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...      1.000000   \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...      1.000000   \n",
       "\n",
       "                                                    Zealot_weight  \n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...            0.0  \n",
       "20d6247127ed58eeca069051672d3e8c3598e132d2fc6c4...            0.0  \n",
       "50cb3b2f33692227e2e50e0e4fab529a44c4bfe9bfd9e81...            1.0  \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...            0.0  \n",
       "eaa36f7eff9c1682c86aace4ff319bce335f3d61c6e3624...            0.0  \n",
       "\n",
       "[5 rows x 106 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = Input(shape=(len(df.columns),))\n",
    "\n",
    "encoded = Dense(64, activation='relu')(input_data)\n",
    "encoded = Dense(32, activation='relu')(input_data)\n",
    "encoded = Dense(24, activation='relu')(input_data)\n",
    "\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_data)\n",
    "               #activity_regularizer=regularizers.l1(10e-3))\n",
    "\n",
    "\n",
    "decoded = Dense(32, activation='relu')(encoded)\n",
    "#decoded = Dense(32, activation='relu')(decoded)\n",
    "decoded = Dense(64, activation='relu')(decoded)\n",
    "decoded = Dense(len(df.columns), activation='linear')(decoded)\n",
    "\n",
    "autoencoder = Model(input_data, decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(input_data, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adadelta', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 2705 samples. Testing on 301.\n"
     ]
    }
   ],
   "source": [
    "x_all = df.sample(frac=1.0).values\n",
    "num_samples = int(0.9 * x_all.shape[0])\n",
    "x_train, x_test = x_all[:num_samples, :], x_all[num_samples:, :]\n",
    "print(\"Training on {} samples. Testing on {}.\".format(\n",
    "    num_samples, x_all.shape[0] - num_samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2705 samples, validate on 301 samples\n",
      "Epoch 1/2000\n",
      "2705/2705 [==============================] - 0s 68us/step - loss: 8881068.0651 - val_loss: 7595721.0449\n",
      "Epoch 2/2000\n",
      "2705/2705 [==============================] - 0s 12us/step - loss: 6404660.9570 - val_loss: 1848337.3256\n",
      "Epoch 3/2000\n",
      "2705/2705 [==============================] - 0s 11us/step - loss: 1520469.1705 - val_loss: 839776.8125\n",
      "Epoch 4/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 801602.4680 - val_loss: 518443.1275\n",
      "Epoch 5/2000\n",
      "2705/2705 [==============================] - 0s 11us/step - loss: 344687.3947 - val_loss: 305276.5023\n",
      "Epoch 6/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 200025.6724 - val_loss: 76900.1343\n",
      "Epoch 7/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 58524.8827 - val_loss: 55400.2956\n",
      "Epoch 8/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 66203.4702 - val_loss: 40556.3930\n",
      "Epoch 9/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 47369.1671 - val_loss: 64837.7688\n",
      "Epoch 10/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 57272.4228 - val_loss: 49575.4350\n",
      "Epoch 11/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 49940.3342 - val_loss: 24418.6538\n",
      "Epoch 12/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 21177.5365 - val_loss: 16195.4147\n",
      "Epoch 13/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14718.1142 - val_loss: 15985.0640\n",
      "Epoch 14/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 17180.4861 - val_loss: 12223.1780\n",
      "Epoch 15/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13870.1782 - val_loss: 14183.1205\n",
      "Epoch 16/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15039.0481 - val_loss: 19618.5458\n",
      "Epoch 17/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 19300.7570 - val_loss: 16091.8158\n",
      "Epoch 18/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 19644.6066 - val_loss: 9883.9778\n",
      "Epoch 19/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 14391.3242 - val_loss: 17624.9650\n",
      "Epoch 20/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 25473.3226 - val_loss: 22870.2124\n",
      "Epoch 21/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 28427.7111 - val_loss: 21984.7432\n",
      "Epoch 22/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 20745.5890 - val_loss: 20068.8941\n",
      "Epoch 23/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15785.5225 - val_loss: 22724.1708\n",
      "Epoch 24/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 20179.3467 - val_loss: 20524.7426\n",
      "Epoch 25/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 20894.2794 - val_loss: 13847.3724\n",
      "Epoch 26/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 17258.8036 - val_loss: 13988.5218\n",
      "Epoch 27/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18603.7537 - val_loss: 17276.1325\n",
      "Epoch 28/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 23697.8064 - val_loss: 34705.4607\n",
      "Epoch 29/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 30956.6718 - val_loss: 28055.2306\n",
      "Epoch 30/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 23854.7932 - val_loss: 23485.4042\n",
      "Epoch 31/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16976.4533 - val_loss: 16661.3140\n",
      "Epoch 32/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17798.5003 - val_loss: 11753.3974\n",
      "Epoch 33/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13425.5507 - val_loss: 8476.0584\n",
      "Epoch 34/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13408.0596 - val_loss: 17176.7775\n",
      "Epoch 35/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 21875.8666 - val_loss: 19980.4666\n",
      "Epoch 36/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 23878.3657 - val_loss: 21072.3968\n",
      "Epoch 37/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 23225.7125 - val_loss: 20947.2495\n",
      "Epoch 38/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 25363.5271 - val_loss: 21613.2449\n",
      "Epoch 39/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 20417.0819 - val_loss: 24925.4461\n",
      "Epoch 40/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 22529.4808 - val_loss: 30368.0402\n",
      "Epoch 41/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 26589.1154 - val_loss: 14176.1215\n",
      "Epoch 42/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 18622.9421 - val_loss: 16295.1827\n",
      "Epoch 43/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 19217.3616 - val_loss: 18611.7567\n",
      "Epoch 44/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 23324.2552 - val_loss: 18610.4405\n",
      "Epoch 45/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 19598.3323 - val_loss: 16458.4724\n",
      "Epoch 46/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 20363.9003 - val_loss: 19137.7638\n",
      "Epoch 47/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 20381.3147 - val_loss: 23196.6145\n",
      "Epoch 48/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 24080.9365 - val_loss: 24353.1806\n",
      "Epoch 49/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 24855.4247 - val_loss: 17318.5823\n",
      "Epoch 50/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 21906.9785 - val_loss: 16914.8927\n",
      "Epoch 51/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 20813.0539 - val_loss: 13869.1661\n",
      "Epoch 52/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18696.8092 - val_loss: 22659.8531\n",
      "Epoch 53/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 22476.0391 - val_loss: 17120.9806\n",
      "Epoch 54/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 22053.6922 - val_loss: 25806.1114\n",
      "Epoch 55/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 24071.9822 - val_loss: 30938.9476\n",
      "Epoch 56/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 33028.1345 - val_loss: 24916.3601\n",
      "Epoch 57/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 26157.8324 - val_loss: 20862.7810\n",
      "Epoch 58/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 21595.8006 - val_loss: 31207.7271\n",
      "Epoch 59/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 26003.4025 - val_loss: 18938.0893\n",
      "Epoch 60/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 21261.8768 - val_loss: 20984.7196\n",
      "Epoch 61/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 22972.8567 - val_loss: 17033.1554\n",
      "Epoch 62/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18828.5566 - val_loss: 14522.1103\n",
      "Epoch 63/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15051.0490 - val_loss: 20247.9360\n",
      "Epoch 64/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 22262.8786 - val_loss: 18892.2819\n",
      "Epoch 65/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 18637.9359 - val_loss: 16454.1747\n",
      "Epoch 66/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 19345.9101 - val_loss: 18137.4329\n",
      "Epoch 67/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16205.9313 - val_loss: 24921.8688\n",
      "Epoch 68/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 23949.3530 - val_loss: 20712.0807\n",
      "Epoch 69/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18125.8194 - val_loss: 26100.4920\n",
      "Epoch 70/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 27031.0050 - val_loss: 19445.4463\n",
      "Epoch 71/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17428.2860 - val_loss: 18990.0733\n",
      "Epoch 72/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 23065.5471 - val_loss: 16580.3830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 20868.9774 - val_loss: 17513.5429\n",
      "Epoch 74/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 21895.4400 - val_loss: 27180.7059\n",
      "Epoch 75/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 25903.9036 - val_loss: 22537.8045\n",
      "Epoch 76/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 23712.7359 - val_loss: 34782.4969\n",
      "Epoch 77/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 32797.6697 - val_loss: 14883.9174\n",
      "Epoch 78/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 18994.2691 - val_loss: 27577.0846\n",
      "Epoch 79/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 26344.4687 - val_loss: 18748.1356\n",
      "Epoch 80/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 22227.1248 - val_loss: 28538.4677\n",
      "Epoch 81/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 28093.5932 - val_loss: 18650.0930\n",
      "Epoch 82/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 24668.2377 - val_loss: 18927.7065\n",
      "Epoch 83/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18226.2307 - val_loss: 17646.1311\n",
      "Epoch 84/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 20396.9190 - val_loss: 20625.5727\n",
      "Epoch 85/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 19559.9092 - val_loss: 19443.8547\n",
      "Epoch 86/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 20131.7901 - val_loss: 24245.8359\n",
      "Epoch 87/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 21234.5978 - val_loss: 20515.3052\n",
      "Epoch 88/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 19500.2467 - val_loss: 23059.7111\n",
      "Epoch 89/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 20010.8976 - val_loss: 19123.3566\n",
      "Epoch 90/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 22366.2067 - val_loss: 20290.8238\n",
      "Epoch 91/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 24957.0580 - val_loss: 11228.1603\n",
      "Epoch 92/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15539.0352 - val_loss: 21137.8217\n",
      "Epoch 93/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 20592.1834 - val_loss: 23092.8878\n",
      "Epoch 94/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 24844.6437 - val_loss: 24533.9244\n",
      "Epoch 95/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 21826.9034 - val_loss: 17916.5653\n",
      "Epoch 96/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 24948.1200 - val_loss: 18744.3604\n",
      "Epoch 97/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 22636.8193 - val_loss: 15938.3686\n",
      "Epoch 98/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18169.4179 - val_loss: 31804.6352\n",
      "Epoch 99/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 28326.2280 - val_loss: 23028.2553\n",
      "Epoch 100/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 27669.6713 - val_loss: 19200.3698\n",
      "Epoch 101/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 23614.3020 - val_loss: 12710.3771\n",
      "Epoch 102/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 19177.6480 - val_loss: 22923.7589\n",
      "Epoch 103/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 23743.4601 - val_loss: 18140.1620\n",
      "Epoch 104/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 24502.1412 - val_loss: 18113.5508\n",
      "Epoch 105/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 21408.5762 - val_loss: 16290.3881\n",
      "Epoch 106/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 19465.3837 - val_loss: 20625.4947\n",
      "Epoch 107/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 22291.0380 - val_loss: 26473.7937\n",
      "Epoch 108/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 29199.5112 - val_loss: 21069.4447\n",
      "Epoch 109/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 21182.9966 - val_loss: 23941.1438\n",
      "Epoch 110/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 26573.5028 - val_loss: 31441.1693\n",
      "Epoch 111/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 28101.9241 - val_loss: 19991.5767\n",
      "Epoch 112/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 20461.7294 - val_loss: 25234.4711\n",
      "Epoch 113/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 24621.0132 - val_loss: 17323.3317\n",
      "Epoch 114/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 22454.2755 - val_loss: 20673.3243\n",
      "Epoch 115/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 22515.0971 - val_loss: 18450.2866\n",
      "Epoch 116/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 21594.7535 - val_loss: 21799.3606\n",
      "Epoch 117/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 23830.2974 - val_loss: 20061.9360\n",
      "Epoch 118/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 23780.1694 - val_loss: 26902.8451\n",
      "Epoch 119/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 25264.8279 - val_loss: 17172.4296\n",
      "Epoch 120/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 19646.1436 - val_loss: 23885.6483\n",
      "Epoch 121/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 22658.6541 - val_loss: 23222.2684\n",
      "Epoch 122/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 27566.6764 - val_loss: 20725.9029\n",
      "Epoch 123/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 21478.6007 - val_loss: 15464.2184\n",
      "Epoch 124/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 18488.3423 - val_loss: 22848.1859\n",
      "Epoch 125/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 22198.5639 - val_loss: 24898.3489\n",
      "Epoch 126/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 24977.1047 - val_loss: 23879.9825\n",
      "Epoch 127/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 24651.0962 - val_loss: 17289.2312\n",
      "Epoch 128/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 21210.7715 - val_loss: 23636.4721\n",
      "Epoch 129/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 25499.8702 - val_loss: 19465.9125\n",
      "Epoch 130/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 23440.6904 - val_loss: 20169.7908\n",
      "Epoch 131/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 21683.1544 - val_loss: 15675.3986\n",
      "Epoch 132/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 20753.0258 - val_loss: 20578.5167\n",
      "Epoch 133/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17793.3844 - val_loss: 28260.2670\n",
      "Epoch 134/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 29248.4213 - val_loss: 28469.9237\n",
      "Epoch 135/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 29270.5277 - val_loss: 15998.9948\n",
      "Epoch 136/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 19223.0984 - val_loss: 33617.5837\n",
      "Epoch 137/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 29266.0545 - val_loss: 19043.5323\n",
      "Epoch 138/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 22466.6974 - val_loss: 24435.3830\n",
      "Epoch 139/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 22896.4494 - val_loss: 28513.9264\n",
      "Epoch 140/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 31616.2181 - val_loss: 14570.4968\n",
      "Epoch 141/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17857.4997 - val_loss: 17728.4102\n",
      "Epoch 142/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 19751.6088 - val_loss: 23126.4213\n",
      "Epoch 143/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 21485.2763 - val_loss: 17411.9809\n",
      "Epoch 144/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18323.2190 - val_loss: 19886.3276\n",
      "Epoch 145/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2705/2705 [==============================] - 0s 9us/step - loss: 18671.2362 - val_loss: 17742.9623\n",
      "Epoch 146/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 20661.8599 - val_loss: 16098.7857\n",
      "Epoch 147/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18212.9744 - val_loss: 14350.2979\n",
      "Epoch 148/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 17715.1675 - val_loss: 20789.1936\n",
      "Epoch 149/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 21027.1889 - val_loss: 19987.6455\n",
      "Epoch 150/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 24897.6302 - val_loss: 21801.8353\n",
      "Epoch 151/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 22781.6284 - val_loss: 22150.2372\n",
      "Epoch 152/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 23470.7732 - val_loss: 29527.9701\n",
      "Epoch 153/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 27234.7440 - val_loss: 21821.3953\n",
      "Epoch 154/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 22390.5228 - val_loss: 28109.4995\n",
      "Epoch 155/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 22035.6059 - val_loss: 22344.5752\n",
      "Epoch 156/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 26731.9913 - val_loss: 23193.6843\n",
      "Epoch 157/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 24539.9008 - val_loss: 21853.8429\n",
      "Epoch 158/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 25638.9411 - val_loss: 24455.6689\n",
      "Epoch 159/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 25831.6114 - val_loss: 20979.7862\n",
      "Epoch 160/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 26493.6504 - val_loss: 24162.0156\n",
      "Epoch 161/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 24664.0926 - val_loss: 19556.8056\n",
      "Epoch 162/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 23795.5035 - val_loss: 29651.1384\n",
      "Epoch 163/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 28261.4110 - val_loss: 18348.9560\n",
      "Epoch 164/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 24408.1125 - val_loss: 22571.8303\n",
      "Epoch 165/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 22177.5839 - val_loss: 17478.2212\n",
      "Epoch 166/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 19476.9147 - val_loss: 27493.0016\n",
      "Epoch 167/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 25950.1736 - val_loss: 22907.9717\n",
      "Epoch 168/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 25711.8256 - val_loss: 22411.9062\n",
      "Epoch 169/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 23979.3356 - val_loss: 16541.2829\n",
      "Epoch 170/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 20744.1109 - val_loss: 24429.1635\n",
      "Epoch 171/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 23640.8351 - val_loss: 22278.2171\n",
      "Epoch 172/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 27721.3263 - val_loss: 19081.1682\n",
      "Epoch 173/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 25114.1638 - val_loss: 18848.0852\n",
      "Epoch 174/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 26364.2701 - val_loss: 25493.8978\n",
      "Epoch 175/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 26533.7065 - val_loss: 24421.2709\n",
      "Epoch 176/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 31597.5036 - val_loss: 28428.5146\n",
      "Epoch 177/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 31761.0958 - val_loss: 20213.8597\n",
      "Epoch 178/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 24425.5800 - val_loss: 24709.2712\n",
      "Epoch 179/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 26336.4705 - val_loss: 19968.4241\n",
      "Epoch 180/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 25348.1586 - val_loss: 24639.0606\n",
      "Epoch 181/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 25629.4824 - val_loss: 19357.1552\n",
      "Epoch 182/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 23892.8654 - val_loss: 24492.4498\n",
      "Epoch 183/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 28314.5732 - val_loss: 13182.0146\n",
      "Epoch 184/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 22361.1310 - val_loss: 17041.2376\n",
      "Epoch 185/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 23567.3207 - val_loss: 13862.8058\n",
      "Epoch 186/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 21204.7210 - val_loss: 19322.3761\n",
      "Epoch 187/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 21892.8740 - val_loss: 21628.4681\n",
      "Epoch 188/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 24651.7745 - val_loss: 27055.6936\n",
      "Epoch 189/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 28018.1403 - val_loss: 21717.9069\n",
      "Epoch 190/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 26515.1385 - val_loss: 19561.9454\n",
      "Epoch 191/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 20923.8799 - val_loss: 20717.2768\n",
      "Epoch 192/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 26012.0716 - val_loss: 23032.7222\n",
      "Epoch 193/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 19631.5012 - val_loss: 24777.3280\n",
      "Epoch 194/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 22886.6853 - val_loss: 34838.7620\n",
      "Epoch 195/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 30342.7611 - val_loss: 16199.7849\n",
      "Epoch 196/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16492.3944 - val_loss: 26728.0133\n",
      "Epoch 197/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 22787.1702 - val_loss: 15696.3988\n",
      "Epoch 198/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 19571.4899 - val_loss: 19479.5179\n",
      "Epoch 199/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 19071.4470 - val_loss: 20263.7833\n",
      "Epoch 200/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 22365.2757 - val_loss: 27829.4627\n",
      "Epoch 201/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 24326.2378 - val_loss: 26332.1142\n",
      "Epoch 202/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 24341.8152 - val_loss: 31445.0054\n",
      "Epoch 203/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 25546.3406 - val_loss: 24691.5271\n",
      "Epoch 204/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 26031.2483 - val_loss: 28912.8095\n",
      "Epoch 205/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 30326.5235 - val_loss: 18295.2125\n",
      "Epoch 206/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 24569.6371 - val_loss: 21496.7834\n",
      "Epoch 207/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 23200.7370 - val_loss: 19688.6182\n",
      "Epoch 208/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 25795.4149 - val_loss: 19910.2251\n",
      "Epoch 209/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 20566.4712 - val_loss: 20209.4738\n",
      "Epoch 210/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 23160.5469 - val_loss: 26175.0225\n",
      "Epoch 211/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 25379.7854 - val_loss: 17897.9274\n",
      "Epoch 212/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 22336.5727 - val_loss: 27357.9586\n",
      "Epoch 213/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 26056.3837 - val_loss: 21374.0705\n",
      "Epoch 214/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 26789.6968 - val_loss: 23447.0632\n",
      "Epoch 215/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 24090.1643 - val_loss: 21567.6559\n",
      "Epoch 216/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 25829.2750 - val_loss: 26625.6593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 217/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 25877.1859 - val_loss: 22839.2699\n",
      "Epoch 218/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 24985.7175 - val_loss: 28833.5910\n",
      "Epoch 219/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 27762.3671 - val_loss: 17401.5811\n",
      "Epoch 220/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 20468.4836 - val_loss: 19714.3894\n",
      "Epoch 221/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 21414.3534 - val_loss: 17600.9049\n",
      "Epoch 222/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 21862.0695 - val_loss: 25252.6051\n",
      "Epoch 223/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 25485.6952 - val_loss: 18669.9970\n",
      "Epoch 224/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 20971.7358 - val_loss: 25275.1255\n",
      "Epoch 225/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 25447.8724 - val_loss: 19411.3043\n",
      "Epoch 226/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 24671.3396 - val_loss: 21811.5270\n",
      "Epoch 227/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 23348.5971 - val_loss: 16770.3332\n",
      "Epoch 228/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 20924.8158 - val_loss: 23248.6672\n",
      "Epoch 229/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 25330.0003 - val_loss: 11415.3653\n",
      "Epoch 230/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 13849.7303 - val_loss: 23121.7289\n",
      "Epoch 231/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 21728.2343 - val_loss: 21030.1019\n",
      "Epoch 232/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 25197.4898 - val_loss: 22623.1978\n",
      "Epoch 233/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 22979.1127 - val_loss: 18274.5215\n",
      "Epoch 234/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 20238.8622 - val_loss: 23380.7941\n",
      "Epoch 235/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 23390.1025 - val_loss: 13907.9375\n",
      "Epoch 236/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 22098.2530 - val_loss: 14866.5596\n",
      "Epoch 237/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 19694.1398 - val_loss: 15106.2222\n",
      "Epoch 238/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 19048.5846 - val_loss: 22429.6195\n",
      "Epoch 239/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 23381.3244 - val_loss: 18119.8731\n",
      "Epoch 240/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 21313.2502 - val_loss: 26682.1486\n",
      "Epoch 241/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 22719.8536 - val_loss: 26327.7248\n",
      "Epoch 242/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 28278.0038 - val_loss: 19210.1868\n",
      "Epoch 243/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18126.6002 - val_loss: 16428.0624\n",
      "Epoch 244/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 17825.3643 - val_loss: 24848.9337\n",
      "Epoch 245/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 22593.7917 - val_loss: 16626.6360\n",
      "Epoch 246/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 22049.5129 - val_loss: 16007.2675\n",
      "Epoch 247/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 18249.2678 - val_loss: 15707.5410\n",
      "Epoch 248/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16897.5934 - val_loss: 20281.2382\n",
      "Epoch 249/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18874.9136 - val_loss: 19954.1850\n",
      "Epoch 250/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 20770.9073 - val_loss: 22524.9392\n",
      "Epoch 251/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 19079.8030 - val_loss: 22024.2052\n",
      "Epoch 252/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 22873.0150 - val_loss: 25097.1146\n",
      "Epoch 253/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 29053.7760 - val_loss: 9480.6363\n",
      "Epoch 254/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13278.1128 - val_loss: 15193.9404\n",
      "Epoch 255/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16162.5414 - val_loss: 18279.4779\n",
      "Epoch 256/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 20836.5882 - val_loss: 22033.9005\n",
      "Epoch 257/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 25186.5234 - val_loss: 12806.8378\n",
      "Epoch 258/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18202.1576 - val_loss: 19007.9005\n",
      "Epoch 259/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 20358.5869 - val_loss: 16370.6705\n",
      "Epoch 260/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 21287.3929 - val_loss: 21446.1218\n",
      "Epoch 261/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 21775.1641 - val_loss: 16264.1743\n",
      "Epoch 262/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18412.1580 - val_loss: 23194.2880\n",
      "Epoch 263/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 23672.1363 - val_loss: 11005.1760\n",
      "Epoch 264/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16862.0610 - val_loss: 15657.3514\n",
      "Epoch 265/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14846.3127 - val_loss: 17872.4887\n",
      "Epoch 266/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 20639.3697 - val_loss: 19368.5572\n",
      "Epoch 267/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 17366.0563 - val_loss: 19509.1356\n",
      "Epoch 268/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 18920.7853 - val_loss: 29165.3188\n",
      "Epoch 269/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 25806.4675 - val_loss: 17734.2686\n",
      "Epoch 270/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17107.7485 - val_loss: 23156.9038\n",
      "Epoch 271/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 20798.1825 - val_loss: 19890.6664\n",
      "Epoch 272/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 22899.7040 - val_loss: 18335.8668\n",
      "Epoch 273/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18073.6594 - val_loss: 18693.7088\n",
      "Epoch 274/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 21870.6576 - val_loss: 17721.0387\n",
      "Epoch 275/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 18096.3377 - val_loss: 19814.9646\n",
      "Epoch 276/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 22598.1829 - val_loss: 21631.5275\n",
      "Epoch 277/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 22500.0575 - val_loss: 17339.8145\n",
      "Epoch 278/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18464.6662 - val_loss: 20267.5759\n",
      "Epoch 279/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 19073.7172 - val_loss: 22475.7742\n",
      "Epoch 280/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 25857.2909 - val_loss: 19799.0962\n",
      "Epoch 281/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 20092.8802 - val_loss: 15620.8993\n",
      "Epoch 282/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 18116.8762 - val_loss: 22519.7858\n",
      "Epoch 283/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 22350.1749 - val_loss: 20035.2676\n",
      "Epoch 284/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 20486.2514 - val_loss: 25973.3231\n",
      "Epoch 285/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 25767.9862 - val_loss: 16966.0000\n",
      "Epoch 286/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16170.9325 - val_loss: 24331.2073\n",
      "Epoch 287/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 20616.5607 - val_loss: 17838.0005\n",
      "Epoch 288/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 21970.2248 - val_loss: 11792.9177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 289/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13384.7164 - val_loss: 14450.2728\n",
      "Epoch 290/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 18076.8698 - val_loss: 18836.3457\n",
      "Epoch 291/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 19990.0072 - val_loss: 15624.5218\n",
      "Epoch 292/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18739.7184 - val_loss: 16108.4076\n",
      "Epoch 293/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16264.5568 - val_loss: 21643.1436\n",
      "Epoch 294/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 19984.6307 - val_loss: 26820.4995\n",
      "Epoch 295/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 23601.5858 - val_loss: 15684.9016\n",
      "Epoch 296/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 19587.0912 - val_loss: 19012.9706\n",
      "Epoch 297/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 20477.6821 - val_loss: 16273.6858\n",
      "Epoch 298/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15979.6025 - val_loss: 23975.2935\n",
      "Epoch 299/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 21062.3676 - val_loss: 21858.3451\n",
      "Epoch 300/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 23814.0706 - val_loss: 21283.5684\n",
      "Epoch 301/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 19277.0570 - val_loss: 19971.4168\n",
      "Epoch 302/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 22614.3524 - val_loss: 18654.7877\n",
      "Epoch 303/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18424.1776 - val_loss: 22301.8087\n",
      "Epoch 304/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 22776.3800 - val_loss: 23854.0092\n",
      "Epoch 305/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 20656.0927 - val_loss: 17025.2504\n",
      "Epoch 306/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18259.3182 - val_loss: 25638.9116\n",
      "Epoch 307/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 22721.9400 - val_loss: 15589.3704\n",
      "Epoch 308/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14822.5190 - val_loss: 23290.6382\n",
      "Epoch 309/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 20814.6811 - val_loss: 20657.9675\n",
      "Epoch 310/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 21409.3137 - val_loss: 20391.1715\n",
      "Epoch 311/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 17681.9088 - val_loss: 19490.7106\n",
      "Epoch 312/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 22164.1052 - val_loss: 17269.3985\n",
      "Epoch 313/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 17240.4475 - val_loss: 16830.7034\n",
      "Epoch 314/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 21300.4530 - val_loss: 14699.7294\n",
      "Epoch 315/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 19134.4416 - val_loss: 15280.1500\n",
      "Epoch 316/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 18985.9105 - val_loss: 16878.4953\n",
      "Epoch 317/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 21273.6776 - val_loss: 13432.9396\n",
      "Epoch 318/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15675.0159 - val_loss: 15195.9489\n",
      "Epoch 319/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14782.7496 - val_loss: 15094.0204\n",
      "Epoch 320/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15839.8776 - val_loss: 18062.1846\n",
      "Epoch 321/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 17482.1889 - val_loss: 17410.0125\n",
      "Epoch 322/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 17869.1657 - val_loss: 22326.4818\n",
      "Epoch 323/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 18139.9497 - val_loss: 18118.6031\n",
      "Epoch 324/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 17203.2414 - val_loss: 23842.1138\n",
      "Epoch 325/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 20304.6647 - val_loss: 17348.5325\n",
      "Epoch 326/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 19017.8966 - val_loss: 19606.5678\n",
      "Epoch 327/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18864.9900 - val_loss: 15374.1187\n",
      "Epoch 328/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17730.8286 - val_loss: 18217.0146\n",
      "Epoch 329/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 15373.7180 - val_loss: 16576.9763\n",
      "Epoch 330/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 19820.0022 - val_loss: 15258.3665\n",
      "Epoch 331/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 18168.4280 - val_loss: 13483.1123\n",
      "Epoch 332/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 17130.8879 - val_loss: 14524.6327\n",
      "Epoch 333/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16455.7355 - val_loss: 13629.3970\n",
      "Epoch 334/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16193.8145 - val_loss: 23241.9228\n",
      "Epoch 335/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 20955.7077 - val_loss: 18118.1895\n",
      "Epoch 336/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 22367.3441 - val_loss: 17045.7465\n",
      "Epoch 337/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16027.2821 - val_loss: 17313.7085\n",
      "Epoch 338/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 21194.5432 - val_loss: 15600.6952\n",
      "Epoch 339/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15164.5070 - val_loss: 13677.0245\n",
      "Epoch 340/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16747.1388 - val_loss: 17948.6549\n",
      "Epoch 341/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17880.1033 - val_loss: 15546.6044\n",
      "Epoch 342/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18070.5388 - val_loss: 16239.3977\n",
      "Epoch 343/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16387.6721 - val_loss: 11958.7477\n",
      "Epoch 344/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16228.1559 - val_loss: 19247.2862\n",
      "Epoch 345/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18087.4090 - val_loss: 19790.7057\n",
      "Epoch 346/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 22377.3212 - val_loss: 22330.9000\n",
      "Epoch 347/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 21570.6552 - val_loss: 11662.7769\n",
      "Epoch 348/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14216.5112 - val_loss: 20454.7979\n",
      "Epoch 349/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17790.0619 - val_loss: 20113.1141\n",
      "Epoch 350/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 20825.2042 - val_loss: 18983.5960\n",
      "Epoch 351/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 19554.3243 - val_loss: 14323.5864\n",
      "Epoch 352/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17687.1399 - val_loss: 20567.0798\n",
      "Epoch 353/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 17466.8658 - val_loss: 16621.8072\n",
      "Epoch 354/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 20156.3027 - val_loss: 16267.7272\n",
      "Epoch 355/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15649.0181 - val_loss: 17600.5553\n",
      "Epoch 356/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14182.6092 - val_loss: 26876.6677\n",
      "Epoch 357/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 20301.3707 - val_loss: 17977.6288\n",
      "Epoch 358/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17220.3140 - val_loss: 24559.4393\n",
      "Epoch 359/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 22768.2491 - val_loss: 13034.6834\n",
      "Epoch 360/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18335.6349 - val_loss: 13151.6468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 361/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15858.4836 - val_loss: 11219.3473\n",
      "Epoch 362/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 15822.1875 - val_loss: 13129.5635\n",
      "Epoch 363/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16327.6872 - val_loss: 15476.0441\n",
      "Epoch 364/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17113.4937 - val_loss: 21171.2817\n",
      "Epoch 365/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 19215.2989 - val_loss: 15992.2710\n",
      "Epoch 366/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16747.8052 - val_loss: 18252.9001\n",
      "Epoch 367/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 17898.5791 - val_loss: 14862.9412\n",
      "Epoch 368/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14521.0132 - val_loss: 22071.6088\n",
      "Epoch 369/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 21766.7680 - val_loss: 14292.1900\n",
      "Epoch 370/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16451.5196 - val_loss: 16142.3887\n",
      "Epoch 371/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 18365.5618 - val_loss: 13691.5430\n",
      "Epoch 372/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 17740.3921 - val_loss: 16552.5449\n",
      "Epoch 373/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16164.5078 - val_loss: 17140.4181\n",
      "Epoch 374/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 19537.2765 - val_loss: 17983.0146\n",
      "Epoch 375/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18377.6189 - val_loss: 17901.7368\n",
      "Epoch 376/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18555.6724 - val_loss: 19786.5728\n",
      "Epoch 377/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18892.8276 - val_loss: 17664.6860\n",
      "Epoch 378/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 19925.7893 - val_loss: 21976.2978\n",
      "Epoch 379/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 19971.8747 - val_loss: 16060.3931\n",
      "Epoch 380/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16692.4152 - val_loss: 18720.9365\n",
      "Epoch 381/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14252.1259 - val_loss: 21481.7969\n",
      "Epoch 382/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 20681.9600 - val_loss: 19894.5904\n",
      "Epoch 383/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 20642.3416 - val_loss: 11542.4969\n",
      "Epoch 384/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12156.6225 - val_loss: 16259.9749\n",
      "Epoch 385/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15989.7957 - val_loss: 13705.6074\n",
      "Epoch 386/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16628.0282 - val_loss: 16451.2826\n",
      "Epoch 387/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 17930.7878 - val_loss: 11035.7047\n",
      "Epoch 388/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 13928.1488 - val_loss: 15366.9166\n",
      "Epoch 389/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 19686.4498 - val_loss: 11296.1829\n",
      "Epoch 390/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13093.9413 - val_loss: 14440.3520\n",
      "Epoch 391/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14168.7828 - val_loss: 15671.8431\n",
      "Epoch 392/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17850.9617 - val_loss: 16452.8510\n",
      "Epoch 393/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 17242.3309 - val_loss: 16710.8506\n",
      "Epoch 394/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 17780.3963 - val_loss: 17113.8260\n",
      "Epoch 395/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 19258.5475 - val_loss: 11968.0712\n",
      "Epoch 396/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15525.3527 - val_loss: 13872.5719\n",
      "Epoch 397/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15062.0941 - val_loss: 16234.6391\n",
      "Epoch 398/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 19647.3598 - val_loss: 13002.2184\n",
      "Epoch 399/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12830.0688 - val_loss: 15101.9618\n",
      "Epoch 400/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18764.8745 - val_loss: 17045.7754\n",
      "Epoch 401/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18607.4214 - val_loss: 9358.5688\n",
      "Epoch 402/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12089.0998 - val_loss: 17015.3462\n",
      "Epoch 403/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14846.9463 - val_loss: 16854.4553\n",
      "Epoch 404/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 19531.0077 - val_loss: 17635.3244\n",
      "Epoch 405/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 18719.3087 - val_loss: 14725.3266\n",
      "Epoch 406/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 20676.5515 - val_loss: 14086.7683\n",
      "Epoch 407/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14650.6048 - val_loss: 16769.2170\n",
      "Epoch 408/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 19926.4194 - val_loss: 20254.2600\n",
      "Epoch 409/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 20146.6377 - val_loss: 13162.3700\n",
      "Epoch 410/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 14904.1571 - val_loss: 19761.9776\n",
      "Epoch 411/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 20101.8858 - val_loss: 12854.2381\n",
      "Epoch 412/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16769.8876 - val_loss: 15029.8688\n",
      "Epoch 413/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 15380.7251 - val_loss: 16876.3112\n",
      "Epoch 414/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18450.5227 - val_loss: 15158.2451\n",
      "Epoch 415/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 19014.2762 - val_loss: 9812.4183\n",
      "Epoch 416/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 12193.5020 - val_loss: 15748.6346\n",
      "Epoch 417/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14916.5961 - val_loss: 15913.5672\n",
      "Epoch 418/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 19389.2414 - val_loss: 12645.0170\n",
      "Epoch 419/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13324.6696 - val_loss: 13870.8243\n",
      "Epoch 420/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15501.1464 - val_loss: 15465.9627\n",
      "Epoch 421/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15896.0436 - val_loss: 12348.4424\n",
      "Epoch 422/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14211.1940 - val_loss: 17467.8469\n",
      "Epoch 423/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17169.3314 - val_loss: 11349.5946\n",
      "Epoch 424/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12684.7645 - val_loss: 16560.7227\n",
      "Epoch 425/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 14591.5592 - val_loss: 15585.2452\n",
      "Epoch 426/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 17377.0993 - val_loss: 13879.8372\n",
      "Epoch 427/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12967.8565 - val_loss: 12813.7254\n",
      "Epoch 428/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 14574.6221 - val_loss: 12204.1058\n",
      "Epoch 429/2000\n",
      "2705/2705 [==============================] - 0s 11us/step - loss: 13062.7485 - val_loss: 12570.4692\n",
      "Epoch 430/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 14258.8084 - val_loss: 15176.1263\n",
      "Epoch 431/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12472.6439 - val_loss: 14237.7622\n",
      "Epoch 432/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15116.0078 - val_loss: 19240.7035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 433/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16254.4450 - val_loss: 12466.2838\n",
      "Epoch 434/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11228.0358 - val_loss: 17019.9019\n",
      "Epoch 435/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17488.6007 - val_loss: 11566.9868\n",
      "Epoch 436/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14598.1040 - val_loss: 11185.6017\n",
      "Epoch 437/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13692.5168 - val_loss: 11952.6189\n",
      "Epoch 438/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16069.8935 - val_loss: 11708.7379\n",
      "Epoch 439/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13113.2340 - val_loss: 11646.5617\n",
      "Epoch 440/2000\n",
      "2705/2705 [==============================] - 0s 11us/step - loss: 15168.6686 - val_loss: 14748.9126\n",
      "Epoch 441/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 15929.9260 - val_loss: 16044.6478\n",
      "Epoch 442/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 19739.3413 - val_loss: 12443.1368\n",
      "Epoch 443/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 12756.9781 - val_loss: 16210.6237\n",
      "Epoch 444/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 19528.4823 - val_loss: 12310.5451\n",
      "Epoch 445/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13323.7804 - val_loss: 15325.9780\n",
      "Epoch 446/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 19281.6948 - val_loss: 13589.6626\n",
      "Epoch 447/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14538.7262 - val_loss: 10603.3359\n",
      "Epoch 448/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 14439.5081 - val_loss: 12844.8127\n",
      "Epoch 449/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16326.3602 - val_loss: 8715.0003\n",
      "Epoch 450/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 9354.9924 - val_loss: 18240.4397\n",
      "Epoch 451/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18200.0381 - val_loss: 13248.8046\n",
      "Epoch 452/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13619.7429 - val_loss: 16137.4234\n",
      "Epoch 453/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14399.5024 - val_loss: 15911.8667\n",
      "Epoch 454/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16970.8016 - val_loss: 14799.6811\n",
      "Epoch 455/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13946.9061 - val_loss: 13350.3484\n",
      "Epoch 456/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16018.8828 - val_loss: 16876.0630\n",
      "Epoch 457/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 17055.4682 - val_loss: 14682.5257\n",
      "Epoch 458/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 17763.4626 - val_loss: 20055.2234\n",
      "Epoch 459/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 20756.0898 - val_loss: 15741.5355\n",
      "Epoch 460/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18160.1724 - val_loss: 16295.8004\n",
      "Epoch 461/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14293.4561 - val_loss: 17533.1737\n",
      "Epoch 462/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 20096.8744 - val_loss: 14599.4711\n",
      "Epoch 463/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15793.4093 - val_loss: 11372.7443\n",
      "Epoch 464/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14991.7787 - val_loss: 14945.4053\n",
      "Epoch 465/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15386.4971 - val_loss: 12170.3472\n",
      "Epoch 466/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14163.2943 - val_loss: 15757.6744\n",
      "Epoch 467/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15968.9097 - val_loss: 13430.7482\n",
      "Epoch 468/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15787.2036 - val_loss: 15287.9175\n",
      "Epoch 469/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14832.7635 - val_loss: 14634.0484\n",
      "Epoch 470/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15819.1575 - val_loss: 17228.0351\n",
      "Epoch 471/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 17460.7082 - val_loss: 14168.6427\n",
      "Epoch 472/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15416.4208 - val_loss: 17043.9746\n",
      "Epoch 473/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 18304.0888 - val_loss: 10817.4605\n",
      "Epoch 474/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13933.7007 - val_loss: 13813.6098\n",
      "Epoch 475/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14745.6138 - val_loss: 14216.9375\n",
      "Epoch 476/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 19746.2227 - val_loss: 12509.1165\n",
      "Epoch 477/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13587.5864 - val_loss: 15299.9835\n",
      "Epoch 478/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15473.3518 - val_loss: 23862.6936\n",
      "Epoch 479/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 20737.9787 - val_loss: 17072.2681\n",
      "Epoch 480/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 19392.9114 - val_loss: 16691.8406\n",
      "Epoch 481/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17836.5846 - val_loss: 11850.2604\n",
      "Epoch 482/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13689.3293 - val_loss: 15183.8505\n",
      "Epoch 483/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16444.4733 - val_loss: 12326.1516\n",
      "Epoch 484/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13398.3122 - val_loss: 19649.2385\n",
      "Epoch 485/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 18193.7447 - val_loss: 15740.2281\n",
      "Epoch 486/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 17318.2035 - val_loss: 23510.4231\n",
      "Epoch 487/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 19350.6983 - val_loss: 14965.5384\n",
      "Epoch 488/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16466.3301 - val_loss: 16419.3596\n",
      "Epoch 489/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16418.9020 - val_loss: 13908.1403\n",
      "Epoch 490/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 16053.1545 - val_loss: 20327.1892\n",
      "Epoch 491/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 17514.0750 - val_loss: 19766.5227\n",
      "Epoch 492/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 20784.4610 - val_loss: 14674.0137\n",
      "Epoch 493/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15917.1136 - val_loss: 12957.6231\n",
      "Epoch 494/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14442.4968 - val_loss: 16858.5784\n",
      "Epoch 495/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 16489.5449 - val_loss: 13309.2691\n",
      "Epoch 496/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15762.8410 - val_loss: 16011.2009\n",
      "Epoch 497/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16010.5649 - val_loss: 13713.8460\n",
      "Epoch 498/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14351.9741 - val_loss: 17922.5907\n",
      "Epoch 499/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 17716.0821 - val_loss: 16861.7222\n",
      "Epoch 500/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 19420.7512 - val_loss: 19846.3882\n",
      "Epoch 501/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 19087.2950 - val_loss: 14385.9545\n",
      "Epoch 502/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12909.8681 - val_loss: 24862.9526\n",
      "Epoch 503/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 19037.1279 - val_loss: 17195.5622\n",
      "Epoch 504/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17807.0210 - val_loss: 20454.9313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 505/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 18295.4408 - val_loss: 14434.6292\n",
      "Epoch 506/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16362.3262 - val_loss: 22088.9557\n",
      "Epoch 507/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 18532.3210 - val_loss: 15636.0212\n",
      "Epoch 508/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18925.6680 - val_loss: 17013.9577\n",
      "Epoch 509/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 19219.9033 - val_loss: 12885.1099\n",
      "Epoch 510/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 17197.8150 - val_loss: 14138.2485\n",
      "Epoch 511/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16021.8874 - val_loss: 13462.5508\n",
      "Epoch 512/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16051.9159 - val_loss: 18981.8624\n",
      "Epoch 513/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17609.0391 - val_loss: 15889.4885\n",
      "Epoch 514/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15103.3292 - val_loss: 21015.5241\n",
      "Epoch 515/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 19364.4379 - val_loss: 17720.6643\n",
      "Epoch 516/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 20919.7496 - val_loss: 15589.3031\n",
      "Epoch 517/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15165.5695 - val_loss: 19385.6313\n",
      "Epoch 518/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 18467.1289 - val_loss: 22250.8479\n",
      "Epoch 519/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 22159.1653 - val_loss: 7032.0861\n",
      "Epoch 520/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11542.9511 - val_loss: 12673.0431\n",
      "Epoch 521/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15923.6976 - val_loss: 10996.0935\n",
      "Epoch 522/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14042.0879 - val_loss: 14553.2166\n",
      "Epoch 523/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17082.4133 - val_loss: 12242.9348\n",
      "Epoch 524/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14487.2020 - val_loss: 18813.3820\n",
      "Epoch 525/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17657.1317 - val_loss: 13161.6827\n",
      "Epoch 526/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15612.5674 - val_loss: 17236.6236\n",
      "Epoch 527/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17734.8686 - val_loss: 14853.7877\n",
      "Epoch 528/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 17665.1090 - val_loss: 16541.2583\n",
      "Epoch 529/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 20297.0893 - val_loss: 13016.9722\n",
      "Epoch 530/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15968.2764 - val_loss: 15931.6013\n",
      "Epoch 531/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16237.5193 - val_loss: 16469.6255\n",
      "Epoch 532/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 19539.5628 - val_loss: 16258.3563\n",
      "Epoch 533/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 18109.0876 - val_loss: 15423.7269\n",
      "Epoch 534/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18150.7834 - val_loss: 15912.8191\n",
      "Epoch 535/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16423.0883 - val_loss: 15897.1889\n",
      "Epoch 536/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 18553.9784 - val_loss: 17436.9933\n",
      "Epoch 537/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 17068.3231 - val_loss: 19156.2624\n",
      "Epoch 538/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 18231.5606 - val_loss: 23846.6211\n",
      "Epoch 539/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 20111.5642 - val_loss: 15569.4013\n",
      "Epoch 540/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18103.7520 - val_loss: 19695.3491\n",
      "Epoch 541/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 22417.7668 - val_loss: 8004.0834\n",
      "Epoch 542/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12209.0828 - val_loss: 12491.0448\n",
      "Epoch 543/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15890.4942 - val_loss: 15319.8161\n",
      "Epoch 544/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16961.2128 - val_loss: 20933.9575\n",
      "Epoch 545/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 21609.5889 - val_loss: 24897.6731\n",
      "Epoch 546/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 20147.8487 - val_loss: 18561.9938\n",
      "Epoch 547/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 17945.6025 - val_loss: 14664.7580\n",
      "Epoch 548/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15081.0515 - val_loss: 18217.9567\n",
      "Epoch 549/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17590.9866 - val_loss: 10733.7545\n",
      "Epoch 550/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14971.4822 - val_loss: 13974.8121\n",
      "Epoch 551/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15229.3176 - val_loss: 13091.6902\n",
      "Epoch 552/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15599.6061 - val_loss: 16445.3841\n",
      "Epoch 553/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 19450.9942 - val_loss: 10628.0815\n",
      "Epoch 554/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16406.2627 - val_loss: 11358.4259\n",
      "Epoch 555/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12757.0902 - val_loss: 14283.4196\n",
      "Epoch 556/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15855.7277 - val_loss: 15138.7072\n",
      "Epoch 557/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15815.0126 - val_loss: 15456.2061\n",
      "Epoch 558/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17310.9678 - val_loss: 18231.0120\n",
      "Epoch 559/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16498.7030 - val_loss: 16135.3150\n",
      "Epoch 560/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18666.9217 - val_loss: 15309.4578\n",
      "Epoch 561/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17431.3902 - val_loss: 9985.3674\n",
      "Epoch 562/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13137.1204 - val_loss: 15782.9509\n",
      "Epoch 563/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15014.2577 - val_loss: 13963.3625\n",
      "Epoch 564/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 15641.6926 - val_loss: 20214.3024\n",
      "Epoch 565/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15805.6940 - val_loss: 13535.3613\n",
      "Epoch 566/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16150.1932 - val_loss: 16862.8946\n",
      "Epoch 567/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 16413.5931 - val_loss: 15862.6161\n",
      "Epoch 568/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17238.4984 - val_loss: 20298.5445\n",
      "Epoch 569/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 18476.8869 - val_loss: 16247.5834\n",
      "Epoch 570/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16986.6701 - val_loss: 18581.1137\n",
      "Epoch 571/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 18876.7438 - val_loss: 12091.7073\n",
      "Epoch 572/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13823.1991 - val_loss: 16340.6105\n",
      "Epoch 573/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16491.5328 - val_loss: 13060.3156\n",
      "Epoch 574/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13960.8482 - val_loss: 18718.6696\n",
      "Epoch 575/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17834.3963 - val_loss: 15584.0073\n",
      "Epoch 576/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 20315.0033 - val_loss: 13225.9817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 577/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 19634.4762 - val_loss: 8498.0660\n",
      "Epoch 578/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12765.3365 - val_loss: 13050.0877\n",
      "Epoch 579/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 14786.2522 - val_loss: 12991.3706\n",
      "Epoch 580/2000\n",
      "2705/2705 [==============================] - 0s 11us/step - loss: 15400.5408 - val_loss: 15358.0580\n",
      "Epoch 581/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 15933.9034 - val_loss: 14150.7397\n",
      "Epoch 582/2000\n",
      "2705/2705 [==============================] - 0s 13us/step - loss: 16029.7764 - val_loss: 17241.3378\n",
      "Epoch 583/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15924.6641 - val_loss: 19487.7113\n",
      "Epoch 584/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 19661.3365 - val_loss: 18786.4768\n",
      "Epoch 585/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 19091.4552 - val_loss: 11745.7487\n",
      "Epoch 586/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 15782.8313 - val_loss: 14583.6496\n",
      "Epoch 587/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 19036.0624 - val_loss: 9357.6821\n",
      "Epoch 588/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12121.4119 - val_loss: 17050.8437\n",
      "Epoch 589/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15458.4962 - val_loss: 16478.4911\n",
      "Epoch 590/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 18545.8578 - val_loss: 12701.3561\n",
      "Epoch 591/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15558.2905 - val_loss: 8300.2977\n",
      "Epoch 592/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11741.7133 - val_loss: 11847.2178\n",
      "Epoch 593/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14030.7380 - val_loss: 12177.9834\n",
      "Epoch 594/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15791.6842 - val_loss: 13032.7778\n",
      "Epoch 595/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14598.4639 - val_loss: 13168.7804\n",
      "Epoch 596/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16509.5078 - val_loss: 18123.5060\n",
      "Epoch 597/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 22860.3555 - val_loss: 14686.6343\n",
      "Epoch 598/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17214.6335 - val_loss: 16042.5669\n",
      "Epoch 599/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16364.4454 - val_loss: 14300.2535\n",
      "Epoch 600/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17796.0838 - val_loss: 19374.3123\n",
      "Epoch 601/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 20633.0262 - val_loss: 16769.1051\n",
      "Epoch 602/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 23613.2280 - val_loss: 12684.2364\n",
      "Epoch 603/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16768.5967 - val_loss: 11722.6943\n",
      "Epoch 604/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14860.0679 - val_loss: 18594.4062\n",
      "Epoch 605/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18708.0869 - val_loss: 20382.7931\n",
      "Epoch 606/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 22940.0924 - val_loss: 16498.8228\n",
      "Epoch 607/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17281.1540 - val_loss: 14686.7386\n",
      "Epoch 608/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18056.0845 - val_loss: 16653.7408\n",
      "Epoch 609/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 16335.0633 - val_loss: 18487.1627\n",
      "Epoch 610/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 21556.0034 - val_loss: 15046.2615\n",
      "Epoch 611/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15507.2679 - val_loss: 12945.8404\n",
      "Epoch 612/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15221.7045 - val_loss: 18674.8032\n",
      "Epoch 613/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 16794.4501 - val_loss: 19505.8562\n",
      "Epoch 614/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 20334.7041 - val_loss: 16444.9668\n",
      "Epoch 615/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13856.5815 - val_loss: 16867.9677\n",
      "Epoch 616/2000\n",
      "2705/2705 [==============================] - 0s 11us/step - loss: 15471.3482 - val_loss: 20715.2836\n",
      "Epoch 617/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 19334.4727 - val_loss: 12107.9348\n",
      "Epoch 618/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14869.7022 - val_loss: 15133.2886\n",
      "Epoch 619/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17608.5472 - val_loss: 11308.7659\n",
      "Epoch 620/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12916.4644 - val_loss: 17425.5113\n",
      "Epoch 621/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 14416.8130 - val_loss: 16916.9759\n",
      "Epoch 622/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16482.5503 - val_loss: 17642.4489\n",
      "Epoch 623/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 17112.7724 - val_loss: 14558.1418\n",
      "Epoch 624/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15903.4981 - val_loss: 17878.2392\n",
      "Epoch 625/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16293.0769 - val_loss: 14924.6400\n",
      "Epoch 626/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12552.6926 - val_loss: 21393.9725\n",
      "Epoch 627/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18531.9296 - val_loss: 15991.0588\n",
      "Epoch 628/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17324.1603 - val_loss: 16267.7832\n",
      "Epoch 629/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 17733.3137 - val_loss: 12222.0799\n",
      "Epoch 630/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16276.1869 - val_loss: 14309.6069\n",
      "Epoch 631/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14737.2611 - val_loss: 10409.4431\n",
      "Epoch 632/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12920.8814 - val_loss: 16450.0431\n",
      "Epoch 633/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15608.1897 - val_loss: 16772.6228\n",
      "Epoch 634/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 19734.9342 - val_loss: 13229.9299\n",
      "Epoch 635/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13133.9877 - val_loss: 13493.3250\n",
      "Epoch 636/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 18210.2143 - val_loss: 11973.9655\n",
      "Epoch 637/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12917.9280 - val_loss: 10360.5453\n",
      "Epoch 638/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13522.1020 - val_loss: 14632.0509\n",
      "Epoch 639/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15873.9195 - val_loss: 11840.2607\n",
      "Epoch 640/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15959.2308 - val_loss: 14882.4944\n",
      "Epoch 641/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14402.3645 - val_loss: 13163.8923\n",
      "Epoch 642/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16387.0845 - val_loss: 15279.4946\n",
      "Epoch 643/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14843.7355 - val_loss: 13618.1775\n",
      "Epoch 644/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17615.9311 - val_loss: 12804.7037\n",
      "Epoch 645/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15142.0393 - val_loss: 13978.8604\n",
      "Epoch 646/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16852.4466 - val_loss: 16213.3250\n",
      "Epoch 647/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 14499.6666 - val_loss: 15807.3048\n",
      "Epoch 648/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 17498.3093 - val_loss: 14892.8224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 649/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16057.4972 - val_loss: 14390.1422\n",
      "Epoch 650/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15407.4097 - val_loss: 18282.7991\n",
      "Epoch 651/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 17374.4448 - val_loss: 14098.3729\n",
      "Epoch 652/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16181.5057 - val_loss: 17618.9279\n",
      "Epoch 653/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17279.4633 - val_loss: 15309.0036\n",
      "Epoch 654/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15260.9651 - val_loss: 18518.7366\n",
      "Epoch 655/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15155.5199 - val_loss: 18868.7195\n",
      "Epoch 656/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 19233.9247 - val_loss: 17201.9944\n",
      "Epoch 657/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 18135.6160 - val_loss: 11453.0651\n",
      "Epoch 658/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 15169.0823 - val_loss: 12803.7650\n",
      "Epoch 659/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 15209.7729 - val_loss: 11313.8734\n",
      "Epoch 660/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14884.3910 - val_loss: 13431.9977\n",
      "Epoch 661/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12103.1214 - val_loss: 17490.3733\n",
      "Epoch 662/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 17621.4496 - val_loss: 16001.9335\n",
      "Epoch 663/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15728.6362 - val_loss: 14711.7847\n",
      "Epoch 664/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17859.1169 - val_loss: 14432.6566\n",
      "Epoch 665/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15788.3385 - val_loss: 14239.8304\n",
      "Epoch 666/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14871.8300 - val_loss: 16140.3872\n",
      "Epoch 667/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15705.6861 - val_loss: 14911.9300\n",
      "Epoch 668/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 18234.2727 - val_loss: 16979.2699\n",
      "Epoch 669/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 17815.5192 - val_loss: 13465.6210\n",
      "Epoch 670/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18603.6201 - val_loss: 16373.1295\n",
      "Epoch 671/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 18168.5100 - val_loss: 15035.3085\n",
      "Epoch 672/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 19232.3228 - val_loss: 18930.2678\n",
      "Epoch 673/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 19156.3781 - val_loss: 15037.4131\n",
      "Epoch 674/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 20302.1952 - val_loss: 21553.1847\n",
      "Epoch 675/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 22717.1793 - val_loss: 17028.9325\n",
      "Epoch 676/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 20990.0582 - val_loss: 20733.7409\n",
      "Epoch 677/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 19955.1034 - val_loss: 22513.0188\n",
      "Epoch 678/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 23513.7029 - val_loss: 23742.8431\n",
      "Epoch 679/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 22159.6529 - val_loss: 19067.3463\n",
      "Epoch 680/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 21296.0928 - val_loss: 14643.8568\n",
      "Epoch 681/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18661.2185 - val_loss: 13826.0224\n",
      "Epoch 682/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16800.7320 - val_loss: 17092.6507\n",
      "Epoch 683/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 18442.0030 - val_loss: 15703.0495\n",
      "Epoch 684/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17443.3354 - val_loss: 24551.1558\n",
      "Epoch 685/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 23484.8399 - val_loss: 14214.3036\n",
      "Epoch 686/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16838.4666 - val_loss: 21672.7651\n",
      "Epoch 687/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 19941.7817 - val_loss: 17589.1166\n",
      "Epoch 688/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 19700.5223 - val_loss: 20842.5385\n",
      "Epoch 689/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 18090.2322 - val_loss: 16161.0332\n",
      "Epoch 690/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 19612.5472 - val_loss: 16003.0018\n",
      "Epoch 691/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 16679.8352 - val_loss: 17634.1150\n",
      "Epoch 692/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 19669.1865 - val_loss: 18019.9722\n",
      "Epoch 693/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18615.5235 - val_loss: 17076.6406\n",
      "Epoch 694/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18440.3187 - val_loss: 18768.4369\n",
      "Epoch 695/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17254.9920 - val_loss: 17269.8362\n",
      "Epoch 696/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17313.9915 - val_loss: 23396.9991\n",
      "Epoch 697/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 19322.3120 - val_loss: 21056.7307\n",
      "Epoch 698/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 21363.9025 - val_loss: 19241.8594\n",
      "Epoch 699/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 18741.8960 - val_loss: 14574.1601\n",
      "Epoch 700/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16648.3121 - val_loss: 20247.7815\n",
      "Epoch 701/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 20731.1778 - val_loss: 14476.3459\n",
      "Epoch 702/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 17284.8152 - val_loss: 17591.0989\n",
      "Epoch 703/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 17544.4123 - val_loss: 17602.9090\n",
      "Epoch 704/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 18893.5774 - val_loss: 18012.3370\n",
      "Epoch 705/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 17263.6581 - val_loss: 16809.3951\n",
      "Epoch 706/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 18284.4027 - val_loss: 17951.9626\n",
      "Epoch 707/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 19834.5921 - val_loss: 11520.6918\n",
      "Epoch 708/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 18580.6498 - val_loss: 10640.1535\n",
      "Epoch 709/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14319.1892 - val_loss: 11713.1510\n",
      "Epoch 710/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14725.1816 - val_loss: 19741.5471\n",
      "Epoch 711/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 19935.6047 - val_loss: 13392.2540\n",
      "Epoch 712/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16705.0397 - val_loss: 18083.6726\n",
      "Epoch 713/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18383.6456 - val_loss: 11972.2585\n",
      "Epoch 714/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17018.4949 - val_loss: 14832.7852\n",
      "Epoch 715/2000\n",
      "2705/2705 [==============================] - 0s 11us/step - loss: 17502.3664 - val_loss: 12015.1987\n",
      "Epoch 716/2000\n",
      "2705/2705 [==============================] - 0s 11us/step - loss: 14591.0515 - val_loss: 17271.1596\n",
      "Epoch 717/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 17174.8948 - val_loss: 19076.2900\n",
      "Epoch 718/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 20138.5318 - val_loss: 18301.4157\n",
      "Epoch 719/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 19116.5010 - val_loss: 14449.0852\n",
      "Epoch 720/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2705/2705 [==============================] - 0s 9us/step - loss: 16692.7085 - val_loss: 20516.8963\n",
      "Epoch 721/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 21131.1992 - val_loss: 13826.2633\n",
      "Epoch 722/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 16866.4131 - val_loss: 18672.5615\n",
      "Epoch 723/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 16186.9815 - val_loss: 22085.3518\n",
      "Epoch 724/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 25624.4971 - val_loss: 16083.1137\n",
      "Epoch 725/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 17937.1197 - val_loss: 14240.2096\n",
      "Epoch 726/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18801.7185 - val_loss: 13446.9552\n",
      "Epoch 727/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 14494.8021 - val_loss: 17709.8555\n",
      "Epoch 728/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 17233.4838 - val_loss: 27078.9855\n",
      "Epoch 729/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 26605.7883 - val_loss: 13213.1177\n",
      "Epoch 730/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14719.3471 - val_loss: 22529.2004\n",
      "Epoch 731/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 22858.4194 - val_loss: 12218.9002\n",
      "Epoch 732/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16699.1317 - val_loss: 15126.4039\n",
      "Epoch 733/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 17433.1091 - val_loss: 12633.6453\n",
      "Epoch 734/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 15783.0908 - val_loss: 20225.8006\n",
      "Epoch 735/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 20079.6870 - val_loss: 14196.0660\n",
      "Epoch 736/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 17408.5341 - val_loss: 18369.8158\n",
      "Epoch 737/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 21221.3824 - val_loss: 11249.6794\n",
      "Epoch 738/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14002.0096 - val_loss: 16826.2168\n",
      "Epoch 739/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 17462.4823 - val_loss: 13688.3943\n",
      "Epoch 740/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 15848.0335 - val_loss: 18130.6491\n",
      "Epoch 741/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17452.0146 - val_loss: 12070.7691\n",
      "Epoch 742/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14530.6272 - val_loss: 16709.4432\n",
      "Epoch 743/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14438.5392 - val_loss: 17127.5288\n",
      "Epoch 744/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 20995.3724 - val_loss: 13178.3907\n",
      "Epoch 745/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15364.5112 - val_loss: 10930.4457\n",
      "Epoch 746/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14345.1554 - val_loss: 11945.9339\n",
      "Epoch 747/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13509.9487 - val_loss: 14164.3030\n",
      "Epoch 748/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14842.1203 - val_loss: 17598.9665\n",
      "Epoch 749/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15151.1581 - val_loss: 15348.5359\n",
      "Epoch 750/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16497.3479 - val_loss: 15514.5403\n",
      "Epoch 751/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16914.1580 - val_loss: 10762.8411\n",
      "Epoch 752/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10981.8294 - val_loss: 18467.5973\n",
      "Epoch 753/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 18485.2648 - val_loss: 10340.2294\n",
      "Epoch 754/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12067.2137 - val_loss: 14618.3239\n",
      "Epoch 755/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 15073.1951 - val_loss: 12760.0203\n",
      "Epoch 756/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17623.2405 - val_loss: 9983.5528\n",
      "Epoch 757/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11213.7665 - val_loss: 13366.8554\n",
      "Epoch 758/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16494.5211 - val_loss: 13739.7178\n",
      "Epoch 759/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 14732.1107 - val_loss: 11951.9567\n",
      "Epoch 760/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13320.2403 - val_loss: 16905.1328\n",
      "Epoch 761/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17722.8004 - val_loss: 11958.8969\n",
      "Epoch 762/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16062.3824 - val_loss: 11370.7972\n",
      "Epoch 763/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12909.5111 - val_loss: 9904.0268\n",
      "Epoch 764/2000\n",
      "2705/2705 [==============================] - 0s 11us/step - loss: 13451.6806 - val_loss: 12038.6894\n",
      "Epoch 765/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14956.9940 - val_loss: 9903.6728\n",
      "Epoch 766/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 13236.7833 - val_loss: 13878.0743\n",
      "Epoch 767/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 12895.7240 - val_loss: 12224.7614\n",
      "Epoch 768/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14818.5175 - val_loss: 13488.6105\n",
      "Epoch 769/2000\n",
      "2705/2705 [==============================] - 0s 11us/step - loss: 14501.1393 - val_loss: 14383.1404\n",
      "Epoch 770/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15624.7623 - val_loss: 15112.8327\n",
      "Epoch 771/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16724.8121 - val_loss: 13103.9045\n",
      "Epoch 772/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17097.4479 - val_loss: 12910.2153\n",
      "Epoch 773/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13880.7613 - val_loss: 10485.8001\n",
      "Epoch 774/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 11931.4603 - val_loss: 17492.4334\n",
      "Epoch 775/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 17881.9037 - val_loss: 13129.2720\n",
      "Epoch 776/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14203.2066 - val_loss: 16044.5667\n",
      "Epoch 777/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 18401.1504 - val_loss: 6763.7620\n",
      "Epoch 778/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 9666.0000 - val_loss: 11501.9315\n",
      "Epoch 779/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 13729.8379 - val_loss: 10572.8009\n",
      "Epoch 780/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13826.8029 - val_loss: 12813.5599\n",
      "Epoch 781/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13433.9160 - val_loss: 13532.4493\n",
      "Epoch 782/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15671.9080 - val_loss: 15584.1346\n",
      "Epoch 783/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 13704.4244 - val_loss: 17630.7264\n",
      "Epoch 784/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 20652.0912 - val_loss: 12479.3190\n",
      "Epoch 785/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13190.4866 - val_loss: 13358.7381\n",
      "Epoch 786/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15392.8599 - val_loss: 13384.3181\n",
      "Epoch 787/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 15649.3856 - val_loss: 14426.8705\n",
      "Epoch 788/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16297.8830 - val_loss: 17158.1482\n",
      "Epoch 789/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16577.1391 - val_loss: 15161.4628\n",
      "Epoch 790/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 19174.2809 - val_loss: 17966.0062\n",
      "Epoch 791/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18913.7245 - val_loss: 14633.8368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 792/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17997.3242 - val_loss: 20340.3794\n",
      "Epoch 793/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 21105.7643 - val_loss: 17487.7478\n",
      "Epoch 794/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 19221.1152 - val_loss: 18245.1588\n",
      "Epoch 795/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 19752.4561 - val_loss: 13665.7216\n",
      "Epoch 796/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 16606.4421 - val_loss: 17995.0586\n",
      "Epoch 797/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 17689.3557 - val_loss: 16156.3426\n",
      "Epoch 798/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 19629.3266 - val_loss: 15338.2292\n",
      "Epoch 799/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16666.3423 - val_loss: 15937.0492\n",
      "Epoch 800/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18117.2095 - val_loss: 18497.0555\n",
      "Epoch 801/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 18070.0473 - val_loss: 14967.4393\n",
      "Epoch 802/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 19853.1364 - val_loss: 14408.3169\n",
      "Epoch 803/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14906.2197 - val_loss: 13936.9940\n",
      "Epoch 804/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 17173.9125 - val_loss: 16323.6035\n",
      "Epoch 805/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 18024.4750 - val_loss: 12008.0254\n",
      "Epoch 806/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15232.7932 - val_loss: 17886.7471\n",
      "Epoch 807/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 20831.5875 - val_loss: 9923.4231\n",
      "Epoch 808/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14046.8165 - val_loss: 13563.0424\n",
      "Epoch 809/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14202.1822 - val_loss: 12418.7350\n",
      "Epoch 810/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15792.2617 - val_loss: 13806.6931\n",
      "Epoch 811/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15037.9377 - val_loss: 13486.3878\n",
      "Epoch 812/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13868.9959 - val_loss: 19609.5616\n",
      "Epoch 813/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 18572.3871 - val_loss: 13469.2077\n",
      "Epoch 814/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14663.1494 - val_loss: 17970.0925\n",
      "Epoch 815/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15099.1176 - val_loss: 17606.2405\n",
      "Epoch 816/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 21120.5599 - val_loss: 11480.1002\n",
      "Epoch 817/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13885.1139 - val_loss: 8983.7527\n",
      "Epoch 818/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 10074.5957 - val_loss: 14815.0573\n",
      "Epoch 819/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13210.0768 - val_loss: 13148.5412\n",
      "Epoch 820/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14445.8877 - val_loss: 14301.0154\n",
      "Epoch 821/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14792.0693 - val_loss: 12427.0083\n",
      "Epoch 822/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14601.0387 - val_loss: 15464.7465\n",
      "Epoch 823/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 13488.4701 - val_loss: 14180.4215\n",
      "Epoch 824/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15094.6673 - val_loss: 13736.7898\n",
      "Epoch 825/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12621.1886 - val_loss: 12443.8231\n",
      "Epoch 826/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 14812.0820 - val_loss: 12469.1676\n",
      "Epoch 827/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 11769.4908 - val_loss: 10013.8221\n",
      "Epoch 828/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13256.9786 - val_loss: 12693.1474\n",
      "Epoch 829/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14584.1584 - val_loss: 10046.6066\n",
      "Epoch 830/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11614.2113 - val_loss: 15927.4498\n",
      "Epoch 831/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 13701.3848 - val_loss: 14255.6188\n",
      "Epoch 832/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16643.8593 - val_loss: 14566.6313\n",
      "Epoch 833/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14995.6238 - val_loss: 11619.1187\n",
      "Epoch 834/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13130.3398 - val_loss: 14496.1831\n",
      "Epoch 835/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14319.4608 - val_loss: 13039.1466\n",
      "Epoch 836/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14870.3218 - val_loss: 14298.0922\n",
      "Epoch 837/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 10122.9919 - val_loss: 17106.6149\n",
      "Epoch 838/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16225.2928 - val_loss: 16009.8121\n",
      "Epoch 839/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14391.2917 - val_loss: 12573.8044\n",
      "Epoch 840/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13568.2460 - val_loss: 15469.9543\n",
      "Epoch 841/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13261.8120 - val_loss: 17427.6910\n",
      "Epoch 842/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16261.8071 - val_loss: 18440.6266\n",
      "Epoch 843/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17756.4769 - val_loss: 9881.2113\n",
      "Epoch 844/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11358.2836 - val_loss: 13637.2408\n",
      "Epoch 845/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14838.7612 - val_loss: 9595.7583\n",
      "Epoch 846/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13501.2590 - val_loss: 8657.7260\n",
      "Epoch 847/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 8560.8560 - val_loss: 13402.5941\n",
      "Epoch 848/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14288.2929 - val_loss: 13478.7508\n",
      "Epoch 849/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 10137.2009 - val_loss: 15526.4682\n",
      "Epoch 850/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15771.8857 - val_loss: 10549.8979\n",
      "Epoch 851/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 9715.3905 - val_loss: 13238.7803\n",
      "Epoch 852/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13568.4847 - val_loss: 14266.7920\n",
      "Epoch 853/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13405.4542 - val_loss: 9184.9250\n",
      "Epoch 854/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10824.4613 - val_loss: 10677.8629\n",
      "Epoch 855/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 10753.4875 - val_loss: 10269.1353\n",
      "Epoch 856/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 11170.1826 - val_loss: 15263.3039\n",
      "Epoch 857/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 13077.5924 - val_loss: 13509.2883\n",
      "Epoch 858/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14642.5481 - val_loss: 12831.9219\n",
      "Epoch 859/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12291.2115 - val_loss: 11827.4898\n",
      "Epoch 860/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13603.4784 - val_loss: 12175.2417\n",
      "Epoch 861/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13864.6527 - val_loss: 9329.8492\n",
      "Epoch 862/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12095.6299 - val_loss: 13697.9788\n",
      "Epoch 863/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15828.3162 - val_loss: 10680.9832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 864/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13529.5620 - val_loss: 18275.2633\n",
      "Epoch 865/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 18234.1871 - val_loss: 10923.5861\n",
      "Epoch 866/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12837.8600 - val_loss: 19062.4581\n",
      "Epoch 867/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 17764.6919 - val_loss: 15684.3737\n",
      "Epoch 868/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 18359.5023 - val_loss: 14805.7140\n",
      "Epoch 869/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15456.0927 - val_loss: 13974.2138\n",
      "Epoch 870/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14309.1934 - val_loss: 17551.3571\n",
      "Epoch 871/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16331.0987 - val_loss: 14088.5385\n",
      "Epoch 872/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15606.5035 - val_loss: 14806.9867\n",
      "Epoch 873/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14588.9084 - val_loss: 14258.4795\n",
      "Epoch 874/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13459.9785 - val_loss: 15030.9452\n",
      "Epoch 875/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 14844.4024 - val_loss: 9392.8751\n",
      "Epoch 876/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12294.7827 - val_loss: 11761.5490\n",
      "Epoch 877/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14057.5783 - val_loss: 10343.5796\n",
      "Epoch 878/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14996.0496 - val_loss: 9967.6619\n",
      "Epoch 879/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 10779.2859 - val_loss: 13007.6185\n",
      "Epoch 880/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 15966.0569 - val_loss: 12200.2981\n",
      "Epoch 881/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12066.3613 - val_loss: 13155.2425\n",
      "Epoch 882/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15124.5948 - val_loss: 12739.7041\n",
      "Epoch 883/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 12864.8476 - val_loss: 10320.7980\n",
      "Epoch 884/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12107.8527 - val_loss: 12626.8426\n",
      "Epoch 885/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11625.7305 - val_loss: 12177.5467\n",
      "Epoch 886/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14127.6051 - val_loss: 13261.9003\n",
      "Epoch 887/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13219.9399 - val_loss: 11184.3614\n",
      "Epoch 888/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11684.1865 - val_loss: 14360.8684\n",
      "Epoch 889/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 14789.1583 - val_loss: 10387.6217\n",
      "Epoch 890/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13742.0641 - val_loss: 8239.6938\n",
      "Epoch 891/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 8947.4070 - val_loss: 9598.6196\n",
      "Epoch 892/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12984.5556 - val_loss: 9287.5122\n",
      "Epoch 893/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11717.0312 - val_loss: 9305.8619\n",
      "Epoch 894/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12707.5755 - val_loss: 9329.6754\n",
      "Epoch 895/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 9875.0408 - val_loss: 11922.6031\n",
      "Epoch 896/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 12740.2338 - val_loss: 11359.8438\n",
      "Epoch 897/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11452.0301 - val_loss: 12318.2702\n",
      "Epoch 898/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 14531.4829 - val_loss: 12171.5714\n",
      "Epoch 899/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 12756.0431 - val_loss: 11327.9777\n",
      "Epoch 900/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 14115.0666 - val_loss: 10167.7108\n",
      "Epoch 901/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 12788.2356 - val_loss: 8422.9547\n",
      "Epoch 902/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10768.1871 - val_loss: 10618.7950\n",
      "Epoch 903/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 9159.9491 - val_loss: 12281.1092\n",
      "Epoch 904/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 13512.8039 - val_loss: 11838.1265\n",
      "Epoch 905/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11608.3796 - val_loss: 11289.4252\n",
      "Epoch 906/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 11213.3664 - val_loss: 13524.2456\n",
      "Epoch 907/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12244.9821 - val_loss: 12456.3384\n",
      "Epoch 908/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 13350.3313 - val_loss: 11069.3779\n",
      "Epoch 909/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12015.6794 - val_loss: 9149.4904\n",
      "Epoch 910/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10216.1467 - val_loss: 12949.5193\n",
      "Epoch 911/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11990.8808 - val_loss: 14006.1438\n",
      "Epoch 912/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14118.1792 - val_loss: 13482.4157\n",
      "Epoch 913/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12732.7312 - val_loss: 12397.9408\n",
      "Epoch 914/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15239.2728 - val_loss: 10691.0986\n",
      "Epoch 915/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11827.4549 - val_loss: 9501.3265\n",
      "Epoch 916/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12221.9277 - val_loss: 11445.6348\n",
      "Epoch 917/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12304.2005 - val_loss: 10405.1214\n",
      "Epoch 918/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11259.0830 - val_loss: 12243.6901\n",
      "Epoch 919/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11762.6379 - val_loss: 11061.5422\n",
      "Epoch 920/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12793.4129 - val_loss: 13405.8112\n",
      "Epoch 921/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13957.9909 - val_loss: 12412.9641\n",
      "Epoch 922/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15151.2402 - val_loss: 13501.5834\n",
      "Epoch 923/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15832.5934 - val_loss: 12288.6038\n",
      "Epoch 924/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17592.6582 - val_loss: 15985.1692\n",
      "Epoch 925/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 19294.1194 - val_loss: 13615.2872\n",
      "Epoch 926/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16353.9522 - val_loss: 16518.6746\n",
      "Epoch 927/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 18159.4725 - val_loss: 11654.0210\n",
      "Epoch 928/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14349.1076 - val_loss: 13655.1773\n",
      "Epoch 929/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12912.4323 - val_loss: 13900.4005\n",
      "Epoch 930/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13271.4334 - val_loss: 20745.2153\n",
      "Epoch 931/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 19820.9330 - val_loss: 10807.5733\n",
      "Epoch 932/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11476.4671 - val_loss: 14833.2017\n",
      "Epoch 933/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14148.9650 - val_loss: 11620.3230\n",
      "Epoch 934/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13963.7470 - val_loss: 13343.9273\n",
      "Epoch 935/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15130.3834 - val_loss: 9071.7356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 936/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13991.2394 - val_loss: 9092.4365\n",
      "Epoch 937/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10566.1875 - val_loss: 10326.9480\n",
      "Epoch 938/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11737.6813 - val_loss: 14135.7289\n",
      "Epoch 939/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13811.8550 - val_loss: 15651.6684\n",
      "Epoch 940/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15697.3104 - val_loss: 16424.0952\n",
      "Epoch 941/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 13401.9224 - val_loss: 14753.9776\n",
      "Epoch 942/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 15557.0601 - val_loss: 14799.9151\n",
      "Epoch 943/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13733.4448 - val_loss: 10139.3234\n",
      "Epoch 944/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11609.5534 - val_loss: 14615.7635\n",
      "Epoch 945/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12826.1012 - val_loss: 13865.1707\n",
      "Epoch 946/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13753.9107 - val_loss: 17825.6251\n",
      "Epoch 947/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 17653.9530 - val_loss: 10180.6761\n",
      "Epoch 948/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 11923.4427 - val_loss: 11302.5451\n",
      "Epoch 949/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13384.5286 - val_loss: 6793.5272\n",
      "Epoch 950/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11017.9145 - val_loss: 8673.5975\n",
      "Epoch 951/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11560.2726 - val_loss: 6939.7003\n",
      "Epoch 952/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 8720.6741 - val_loss: 11852.4906\n",
      "Epoch 953/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12365.2298 - val_loss: 11209.2752\n",
      "Epoch 954/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13579.6815 - val_loss: 11802.4509\n",
      "Epoch 955/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12505.7434 - val_loss: 11497.4389\n",
      "Epoch 956/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12692.3557 - val_loss: 15213.2248\n",
      "Epoch 957/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13643.5308 - val_loss: 10865.3785\n",
      "Epoch 958/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13281.0477 - val_loss: 11580.6433\n",
      "Epoch 959/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11004.0575 - val_loss: 12833.2072\n",
      "Epoch 960/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 14493.0763 - val_loss: 12263.6738\n",
      "Epoch 961/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11577.2024 - val_loss: 13380.9549\n",
      "Epoch 962/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14518.9448 - val_loss: 12293.4669\n",
      "Epoch 963/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12106.8716 - val_loss: 9840.5142\n",
      "Epoch 964/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11582.2065 - val_loss: 14320.9684\n",
      "Epoch 965/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13459.3367 - val_loss: 11404.0990\n",
      "Epoch 966/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11712.6203 - val_loss: 13785.6850\n",
      "Epoch 967/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12809.9200 - val_loss: 12926.0573\n",
      "Epoch 968/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16242.8990 - val_loss: 10954.8219\n",
      "Epoch 969/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13182.9708 - val_loss: 8883.1663\n",
      "Epoch 970/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12180.9415 - val_loss: 10323.1205\n",
      "Epoch 971/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12707.1098 - val_loss: 11052.2924\n",
      "Epoch 972/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 11095.0287 - val_loss: 16627.6391\n",
      "Epoch 973/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15848.9576 - val_loss: 9365.5680\n",
      "Epoch 974/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11791.7477 - val_loss: 13761.4154\n",
      "Epoch 975/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13817.3646 - val_loss: 11811.8215\n",
      "Epoch 976/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12596.9907 - val_loss: 14951.1416\n",
      "Epoch 977/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13846.2169 - val_loss: 10846.7953\n",
      "Epoch 978/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13723.3456 - val_loss: 11823.3796\n",
      "Epoch 979/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12443.0092 - val_loss: 12012.8709\n",
      "Epoch 980/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14687.6001 - val_loss: 11969.2107\n",
      "Epoch 981/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10996.7472 - val_loss: 13563.6952\n",
      "Epoch 982/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14427.5104 - val_loss: 12415.2971\n",
      "Epoch 983/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 12981.1231 - val_loss: 12273.6154\n",
      "Epoch 984/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14453.6943 - val_loss: 12926.2971\n",
      "Epoch 985/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15506.3798 - val_loss: 10347.8753\n",
      "Epoch 986/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12644.8552 - val_loss: 13811.9035\n",
      "Epoch 987/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14029.5608 - val_loss: 11269.4837\n",
      "Epoch 988/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14372.1451 - val_loss: 13585.6725\n",
      "Epoch 989/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11723.0859 - val_loss: 15228.7678\n",
      "Epoch 990/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17798.8491 - val_loss: 11785.0583\n",
      "Epoch 991/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13371.9739 - val_loss: 11714.9711\n",
      "Epoch 992/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13360.3811 - val_loss: 15153.5987\n",
      "Epoch 993/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14966.5406 - val_loss: 13116.4232\n",
      "Epoch 994/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14950.7290 - val_loss: 13299.7081\n",
      "Epoch 995/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15096.5691 - val_loss: 8177.8071\n",
      "Epoch 996/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11292.0869 - val_loss: 11885.6592\n",
      "Epoch 997/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13790.3156 - val_loss: 10335.5419\n",
      "Epoch 998/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11054.0454 - val_loss: 15887.3344\n",
      "Epoch 999/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15266.2015 - val_loss: 11029.8417\n",
      "Epoch 1000/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13233.5100 - val_loss: 12274.7946\n",
      "Epoch 1001/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13526.5607 - val_loss: 11953.7167\n",
      "Epoch 1002/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14908.3234 - val_loss: 11725.5699\n",
      "Epoch 1003/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 14358.7656 - val_loss: 8721.9538\n",
      "Epoch 1004/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10358.6710 - val_loss: 14557.4148\n",
      "Epoch 1005/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13096.1425 - val_loss: 14546.6925\n",
      "Epoch 1006/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15954.6204 - val_loss: 13491.2135\n",
      "Epoch 1007/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14921.3264 - val_loss: 9391.9359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1008/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12002.1822 - val_loss: 11384.0560\n",
      "Epoch 1009/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12456.0636 - val_loss: 12160.9973\n",
      "Epoch 1010/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14172.3460 - val_loss: 13937.3003\n",
      "Epoch 1011/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14055.8805 - val_loss: 12236.9257\n",
      "Epoch 1012/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12929.2026 - val_loss: 17863.5348\n",
      "Epoch 1013/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14978.0123 - val_loss: 16847.8794\n",
      "Epoch 1014/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14668.1549 - val_loss: 18734.2928\n",
      "Epoch 1015/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15574.3457 - val_loss: 13736.4734\n",
      "Epoch 1016/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14330.4830 - val_loss: 16139.6127\n",
      "Epoch 1017/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15987.8209 - val_loss: 11786.3250\n",
      "Epoch 1018/2000\n",
      "2705/2705 [==============================] - 0s 6us/step - loss: 15902.4337 - val_loss: 15602.7145\n",
      "Epoch 1019/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 14475.1513 - val_loss: 12405.0310\n",
      "Epoch 1020/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 16319.5567 - val_loss: 15829.8718\n",
      "Epoch 1021/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 17874.8214 - val_loss: 14143.3503\n",
      "Epoch 1022/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 19323.0614 - val_loss: 17975.5900\n",
      "Epoch 1023/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 19625.9676 - val_loss: 12022.8372\n",
      "Epoch 1024/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15026.0143 - val_loss: 13391.4786\n",
      "Epoch 1025/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16175.4107 - val_loss: 13294.7140\n",
      "Epoch 1026/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 17625.3747 - val_loss: 10729.1748\n",
      "Epoch 1027/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10916.2760 - val_loss: 12189.5459\n",
      "Epoch 1028/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 13601.1339 - val_loss: 14480.7511\n",
      "Epoch 1029/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11440.4818 - val_loss: 16462.6402\n",
      "Epoch 1030/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15354.8631 - val_loss: 17238.7678\n",
      "Epoch 1031/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16111.7301 - val_loss: 12788.8518\n",
      "Epoch 1032/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14859.6803 - val_loss: 12946.8940\n",
      "Epoch 1033/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13336.2511 - val_loss: 12261.3602\n",
      "Epoch 1034/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14621.9722 - val_loss: 13718.0868\n",
      "Epoch 1035/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14304.9567 - val_loss: 10475.4882\n",
      "Epoch 1036/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13530.9867 - val_loss: 12170.3333\n",
      "Epoch 1037/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11972.0097 - val_loss: 13678.7217\n",
      "Epoch 1038/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16273.5091 - val_loss: 12726.0889\n",
      "Epoch 1039/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11981.1711 - val_loss: 12166.3690\n",
      "Epoch 1040/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12783.6555 - val_loss: 14105.1275\n",
      "Epoch 1041/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15522.5212 - val_loss: 12379.5642\n",
      "Epoch 1042/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14271.2454 - val_loss: 11954.1483\n",
      "Epoch 1043/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13660.8757 - val_loss: 9966.3445\n",
      "Epoch 1044/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 11605.4972 - val_loss: 13968.2828\n",
      "Epoch 1045/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13787.9593 - val_loss: 10769.3448\n",
      "Epoch 1046/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12840.3156 - val_loss: 12062.8467\n",
      "Epoch 1047/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 11864.0657 - val_loss: 9639.6707\n",
      "Epoch 1048/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10369.6428 - val_loss: 14769.9613\n",
      "Epoch 1049/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13568.6886 - val_loss: 13172.2581\n",
      "Epoch 1050/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14422.1724 - val_loss: 11494.2223\n",
      "Epoch 1051/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 11256.7826 - val_loss: 15636.3208\n",
      "Epoch 1052/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15706.7526 - val_loss: 14433.1266\n",
      "Epoch 1053/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14910.4566 - val_loss: 6532.3695\n",
      "Epoch 1054/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 8568.8641 - val_loss: 11426.4047\n",
      "Epoch 1055/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 12393.6856 - val_loss: 9411.5190\n",
      "Epoch 1056/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 9292.9723 - val_loss: 14004.2828\n",
      "Epoch 1057/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14313.8963 - val_loss: 8049.1750\n",
      "Epoch 1058/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10162.4131 - val_loss: 11975.2997\n",
      "Epoch 1059/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11953.1272 - val_loss: 12436.6409\n",
      "Epoch 1060/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13360.7932 - val_loss: 11614.7544\n",
      "Epoch 1061/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12923.0116 - val_loss: 9537.6183\n",
      "Epoch 1062/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10605.3178 - val_loss: 11473.7053\n",
      "Epoch 1063/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11212.6930 - val_loss: 12063.1038\n",
      "Epoch 1064/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13314.1766 - val_loss: 12810.7284\n",
      "Epoch 1065/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13898.8394 - val_loss: 9058.7383\n",
      "Epoch 1066/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11005.6800 - val_loss: 11430.7407\n",
      "Epoch 1067/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 12273.0930 - val_loss: 10001.2368\n",
      "Epoch 1068/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 12519.3909 - val_loss: 11902.9448\n",
      "Epoch 1069/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13087.3769 - val_loss: 10146.1360\n",
      "Epoch 1070/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 9986.9441 - val_loss: 15454.0625\n",
      "Epoch 1071/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 16031.1456 - val_loss: 8173.8956\n",
      "Epoch 1072/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10376.8806 - val_loss: 10300.6299\n",
      "Epoch 1073/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10598.8555 - val_loss: 8676.8727\n",
      "Epoch 1074/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 9458.7226 - val_loss: 13726.2047\n",
      "Epoch 1075/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13048.4403 - val_loss: 9932.6244\n",
      "Epoch 1076/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 10210.7675 - val_loss: 14976.7982\n",
      "Epoch 1077/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12947.6368 - val_loss: 12363.8858\n",
      "Epoch 1078/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13553.9295 - val_loss: 11779.8183\n",
      "Epoch 1079/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2705/2705 [==============================] - 0s 9us/step - loss: 10462.9413 - val_loss: 11102.5972\n",
      "Epoch 1080/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10745.3361 - val_loss: 13919.8592\n",
      "Epoch 1081/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12817.9605 - val_loss: 11456.3048\n",
      "Epoch 1082/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 11553.2691 - val_loss: 12284.0211\n",
      "Epoch 1083/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11601.5039 - val_loss: 10275.0972\n",
      "Epoch 1084/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12149.2423 - val_loss: 13331.1264\n",
      "Epoch 1085/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11849.4209 - val_loss: 12581.5666\n",
      "Epoch 1086/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14903.7067 - val_loss: 10461.0066\n",
      "Epoch 1087/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12155.1908 - val_loss: 10794.4722\n",
      "Epoch 1088/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12015.8242 - val_loss: 15089.2279\n",
      "Epoch 1089/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15530.2750 - val_loss: 11908.3264\n",
      "Epoch 1090/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11992.6253 - val_loss: 16085.0409\n",
      "Epoch 1091/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17116.3334 - val_loss: 9542.3258\n",
      "Epoch 1092/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11541.9308 - val_loss: 11642.9975\n",
      "Epoch 1093/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 11914.6851 - val_loss: 12764.2577\n",
      "Epoch 1094/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12623.7458 - val_loss: 16164.5265\n",
      "Epoch 1095/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15908.1309 - val_loss: 11519.4473\n",
      "Epoch 1096/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11588.2869 - val_loss: 12386.0245\n",
      "Epoch 1097/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12828.8423 - val_loss: 10145.1791\n",
      "Epoch 1098/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11900.2334 - val_loss: 12236.8248\n",
      "Epoch 1099/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10363.0401 - val_loss: 14061.5629\n",
      "Epoch 1100/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14631.2824 - val_loss: 11384.7574\n",
      "Epoch 1101/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11609.5565 - val_loss: 8794.0836\n",
      "Epoch 1102/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11509.5706 - val_loss: 9584.8769\n",
      "Epoch 1103/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 10566.3059 - val_loss: 8332.5632\n",
      "Epoch 1104/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 10924.5477 - val_loss: 10012.1050\n",
      "Epoch 1105/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10993.5274 - val_loss: 9277.4215\n",
      "Epoch 1106/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 10674.7633 - val_loss: 13139.2972\n",
      "Epoch 1107/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 12210.5608 - val_loss: 11618.1919\n",
      "Epoch 1108/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11674.6039 - val_loss: 16617.6584\n",
      "Epoch 1109/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 12660.3774 - val_loss: 11883.6805\n",
      "Epoch 1110/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15024.5221 - val_loss: 11109.5533\n",
      "Epoch 1111/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11628.2903 - val_loss: 10402.8513\n",
      "Epoch 1112/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12387.8539 - val_loss: 11778.4423\n",
      "Epoch 1113/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12297.4249 - val_loss: 8927.1068\n",
      "Epoch 1114/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11185.5454 - val_loss: 11804.3384\n",
      "Epoch 1115/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11361.8109 - val_loss: 10831.2648\n",
      "Epoch 1116/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 12511.7165 - val_loss: 11428.5608\n",
      "Epoch 1117/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11514.6652 - val_loss: 11433.4619\n",
      "Epoch 1118/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12619.3786 - val_loss: 13201.1931\n",
      "Epoch 1119/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13590.1003 - val_loss: 10217.3650\n",
      "Epoch 1120/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 12791.6430 - val_loss: 12415.2685\n",
      "Epoch 1121/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13089.9620 - val_loss: 11013.2732\n",
      "Epoch 1122/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13044.3169 - val_loss: 10961.2012\n",
      "Epoch 1123/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11099.0206 - val_loss: 11533.4054\n",
      "Epoch 1124/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12135.1073 - val_loss: 14746.3214\n",
      "Epoch 1125/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13151.1517 - val_loss: 9544.8273\n",
      "Epoch 1126/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 9706.7551 - val_loss: 14856.4215\n",
      "Epoch 1127/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14335.6955 - val_loss: 10309.3661\n",
      "Epoch 1128/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 12775.0320 - val_loss: 10532.0644\n",
      "Epoch 1129/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 10966.8792 - val_loss: 10148.7314\n",
      "Epoch 1130/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13815.5391 - val_loss: 8975.3974\n",
      "Epoch 1131/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10175.8507 - val_loss: 8747.0267\n",
      "Epoch 1132/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11368.1734 - val_loss: 10729.2964\n",
      "Epoch 1133/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11102.6895 - val_loss: 12021.8183\n",
      "Epoch 1134/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13285.7461 - val_loss: 13603.9460\n",
      "Epoch 1135/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 11680.4146 - val_loss: 12567.2290\n",
      "Epoch 1136/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12157.8369 - val_loss: 15104.4324\n",
      "Epoch 1137/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14807.1646 - val_loss: 10092.7203\n",
      "Epoch 1138/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11990.8431 - val_loss: 12211.4784\n",
      "Epoch 1139/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 13052.6729 - val_loss: 10488.6567\n",
      "Epoch 1140/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11548.3186 - val_loss: 13929.1220\n",
      "Epoch 1141/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13586.4046 - val_loss: 10618.6024\n",
      "Epoch 1142/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12903.9835 - val_loss: 10136.6907\n",
      "Epoch 1143/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 10583.6195 - val_loss: 11158.8663\n",
      "Epoch 1144/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12764.5365 - val_loss: 10979.2039\n",
      "Epoch 1145/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10859.6640 - val_loss: 13549.9873\n",
      "Epoch 1146/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13943.9176 - val_loss: 12403.9716\n",
      "Epoch 1147/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 12251.1509 - val_loss: 10757.4151\n",
      "Epoch 1148/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11847.8704 - val_loss: 14159.1421\n",
      "Epoch 1149/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14616.1891 - val_loss: 14467.5671\n",
      "Epoch 1150/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2705/2705 [==============================] - 0s 7us/step - loss: 18310.3300 - val_loss: 12702.6084\n",
      "Epoch 1151/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14962.5855 - val_loss: 10828.0084\n",
      "Epoch 1152/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12899.1878 - val_loss: 15583.8088\n",
      "Epoch 1153/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14895.0501 - val_loss: 15187.8935\n",
      "Epoch 1154/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 18339.0085 - val_loss: 12237.9126\n",
      "Epoch 1155/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16121.7301 - val_loss: 11065.6299\n",
      "Epoch 1156/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11781.3135 - val_loss: 17726.0577\n",
      "Epoch 1157/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16987.3670 - val_loss: 15665.1730\n",
      "Epoch 1158/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15926.2887 - val_loss: 16634.1151\n",
      "Epoch 1159/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17325.8681 - val_loss: 11364.6431\n",
      "Epoch 1160/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 13928.5769 - val_loss: 16080.2550\n",
      "Epoch 1161/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13690.0670 - val_loss: 18397.7847\n",
      "Epoch 1162/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16773.5016 - val_loss: 18119.9748\n",
      "Epoch 1163/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 17120.9564 - val_loss: 15287.3928\n",
      "Epoch 1164/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16907.6929 - val_loss: 16060.9923\n",
      "Epoch 1165/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 14232.5584 - val_loss: 13554.2032\n",
      "Epoch 1166/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16328.5673 - val_loss: 13636.1208\n",
      "Epoch 1167/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16023.7457 - val_loss: 10014.0659\n",
      "Epoch 1168/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 16786.5918 - val_loss: 8429.4177\n",
      "Epoch 1169/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10731.2424 - val_loss: 10141.3127\n",
      "Epoch 1170/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13035.7712 - val_loss: 11667.6727\n",
      "Epoch 1171/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12045.1981 - val_loss: 13220.0719\n",
      "Epoch 1172/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15188.0552 - val_loss: 15198.1418\n",
      "Epoch 1173/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 15043.3227 - val_loss: 13794.3301\n",
      "Epoch 1174/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 17702.8191 - val_loss: 10726.8869\n",
      "Epoch 1175/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11345.4736 - val_loss: 13371.1875\n",
      "Epoch 1176/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14499.7967 - val_loss: 17741.8714\n",
      "Epoch 1177/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 18013.1306 - val_loss: 10448.7677\n",
      "Epoch 1178/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13165.9836 - val_loss: 12510.5457\n",
      "Epoch 1179/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 10271.6936 - val_loss: 13714.2186\n",
      "Epoch 1180/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 14112.2594 - val_loss: 17076.5531\n",
      "Epoch 1181/2000\n",
      "2705/2705 [==============================] - 0s 11us/step - loss: 16021.7092 - val_loss: 12143.2339\n",
      "Epoch 1182/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14124.3023 - val_loss: 11970.3359\n",
      "Epoch 1183/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 17282.9149 - val_loss: 5925.2063\n",
      "Epoch 1184/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 9523.2290 - val_loss: 9174.9279\n",
      "Epoch 1185/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 9526.9943 - val_loss: 10915.8813\n",
      "Epoch 1186/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 10912.5518 - val_loss: 16333.0474\n",
      "Epoch 1187/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14300.4835 - val_loss: 13206.0070\n",
      "Epoch 1188/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 14421.5364 - val_loss: 12989.5015\n",
      "Epoch 1189/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 11700.4125 - val_loss: 13022.7211\n",
      "Epoch 1190/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14438.0465 - val_loss: 11902.1144\n",
      "Epoch 1191/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13553.4029 - val_loss: 9918.0043\n",
      "Epoch 1192/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 10941.9468 - val_loss: 13766.6392\n",
      "Epoch 1193/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13380.5590 - val_loss: 10041.1742\n",
      "Epoch 1194/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11828.3976 - val_loss: 13853.2645\n",
      "Epoch 1195/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12342.0184 - val_loss: 12529.9601\n",
      "Epoch 1196/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13284.2928 - val_loss: 13727.8069\n",
      "Epoch 1197/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13707.7810 - val_loss: 14793.1383\n",
      "Epoch 1198/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15340.8048 - val_loss: 14166.6970\n",
      "Epoch 1199/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13096.2541 - val_loss: 13287.2778\n",
      "Epoch 1200/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 15294.2787 - val_loss: 11395.2483\n",
      "Epoch 1201/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 10691.0195 - val_loss: 14544.2993\n",
      "Epoch 1202/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16052.4540 - val_loss: 12301.7187\n",
      "Epoch 1203/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13587.3644 - val_loss: 9710.3844\n",
      "Epoch 1204/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 12799.6509 - val_loss: 9832.3721\n",
      "Epoch 1205/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 11840.1847 - val_loss: 9871.1850\n",
      "Epoch 1206/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11104.7678 - val_loss: 15539.0849\n",
      "Epoch 1207/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14717.5479 - val_loss: 10988.9853\n",
      "Epoch 1208/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13058.7552 - val_loss: 11837.8920\n",
      "Epoch 1209/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13894.0197 - val_loss: 6306.2602\n",
      "Epoch 1210/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 9451.4577 - val_loss: 8757.5497\n",
      "Epoch 1211/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 9664.6759 - val_loss: 11899.6071\n",
      "Epoch 1212/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12022.2404 - val_loss: 14040.1478\n",
      "Epoch 1213/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14793.6477 - val_loss: 9942.1518\n",
      "Epoch 1214/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 9826.3460 - val_loss: 15849.0225\n",
      "Epoch 1215/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 14283.0519 - val_loss: 11753.2998\n",
      "Epoch 1216/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 12493.5689 - val_loss: 13445.0944\n",
      "Epoch 1217/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12026.3097 - val_loss: 10695.1572\n",
      "Epoch 1218/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12611.7487 - val_loss: 13947.6715\n",
      "Epoch 1219/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 13969.0659 - val_loss: 11806.8894\n",
      "Epoch 1220/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16151.1592 - val_loss: 8107.1690\n",
      "Epoch 1221/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2705/2705 [==============================] - 0s 8us/step - loss: 9580.9467 - val_loss: 8205.8170\n",
      "Epoch 1222/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11989.4928 - val_loss: 9142.1726\n",
      "Epoch 1223/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 13138.0988 - val_loss: 6356.8993\n",
      "Epoch 1224/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 8862.2411 - val_loss: 9063.7497\n",
      "Epoch 1225/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11128.1437 - val_loss: 9119.8779\n",
      "Epoch 1226/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 11973.4312 - val_loss: 11595.9701\n",
      "Epoch 1227/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 12466.8216 - val_loss: 10594.1930\n",
      "Epoch 1228/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13762.4685 - val_loss: 9988.8961\n",
      "Epoch 1229/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12499.6522 - val_loss: 9702.3185\n",
      "Epoch 1230/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15203.1855 - val_loss: 9978.3241\n",
      "Epoch 1231/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14175.2470 - val_loss: 12626.1217\n",
      "Epoch 1232/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15876.1837 - val_loss: 16281.2845\n",
      "Epoch 1233/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17378.7030 - val_loss: 14050.9861\n",
      "Epoch 1234/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14262.3731 - val_loss: 20722.9648\n",
      "Epoch 1235/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16365.7739 - val_loss: 19386.1739\n",
      "Epoch 1236/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 19217.0103 - val_loss: 15066.6244\n",
      "Epoch 1237/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16641.9262 - val_loss: 10638.5900\n",
      "Epoch 1238/2000\n",
      "2705/2705 [==============================] - 0s 11us/step - loss: 11842.6820 - val_loss: 14240.5947\n",
      "Epoch 1239/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12915.2826 - val_loss: 16437.7617\n",
      "Epoch 1240/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 17563.3836 - val_loss: 12946.3624\n",
      "Epoch 1241/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 14659.4641 - val_loss: 12552.9894\n",
      "Epoch 1242/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12148.2915 - val_loss: 17238.3817\n",
      "Epoch 1243/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13326.7319 - val_loss: 15167.6833\n",
      "Epoch 1244/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16083.6478 - val_loss: 12874.3704\n",
      "Epoch 1245/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13023.3131 - val_loss: 12353.1562\n",
      "Epoch 1246/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15401.0285 - val_loss: 11427.6140\n",
      "Epoch 1247/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11711.8648 - val_loss: 11551.7844\n",
      "Epoch 1248/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11342.8566 - val_loss: 16197.3947\n",
      "Epoch 1249/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13146.4030 - val_loss: 18156.1368\n",
      "Epoch 1250/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16656.1181 - val_loss: 14153.0640\n",
      "Epoch 1251/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11457.0523 - val_loss: 12372.3291\n",
      "Epoch 1252/2000\n",
      "2705/2705 [==============================] - 0s 11us/step - loss: 14358.4875 - val_loss: 13273.5741\n",
      "Epoch 1253/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 12992.0708 - val_loss: 12121.3818\n",
      "Epoch 1254/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13280.1170 - val_loss: 15626.1554\n",
      "Epoch 1255/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15548.2752 - val_loss: 10830.0439\n",
      "Epoch 1256/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12769.9579 - val_loss: 9185.5137\n",
      "Epoch 1257/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11090.4939 - val_loss: 9919.6317\n",
      "Epoch 1258/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12714.0590 - val_loss: 12895.1868\n",
      "Epoch 1259/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11665.9374 - val_loss: 13772.0569\n",
      "Epoch 1260/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14047.7227 - val_loss: 15507.6482\n",
      "Epoch 1261/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 14649.7799 - val_loss: 11035.2333\n",
      "Epoch 1262/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11479.5187 - val_loss: 15524.0011\n",
      "Epoch 1263/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15435.9658 - val_loss: 10499.2623\n",
      "Epoch 1264/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 12848.5863 - val_loss: 11707.4153\n",
      "Epoch 1265/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11485.1265 - val_loss: 14026.9675\n",
      "Epoch 1266/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15878.7402 - val_loss: 13452.4913\n",
      "Epoch 1267/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12838.8083 - val_loss: 10168.7802\n",
      "Epoch 1268/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 12535.6736 - val_loss: 11800.7915\n",
      "Epoch 1269/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11803.0913 - val_loss: 14149.2586\n",
      "Epoch 1270/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13216.2316 - val_loss: 18601.4573\n",
      "Epoch 1271/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16350.2086 - val_loss: 11601.7717\n",
      "Epoch 1272/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12912.2333 - val_loss: 10683.5158\n",
      "Epoch 1273/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12916.6492 - val_loss: 8996.1684\n",
      "Epoch 1274/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12770.9691 - val_loss: 10247.0125\n",
      "Epoch 1275/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11023.2185 - val_loss: 9659.1991\n",
      "Epoch 1276/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12583.0302 - val_loss: 10998.6597\n",
      "Epoch 1277/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10077.9387 - val_loss: 10930.6547\n",
      "Epoch 1278/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14319.3980 - val_loss: 11753.3835\n",
      "Epoch 1279/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 9991.7824 - val_loss: 14422.0189\n",
      "Epoch 1280/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12419.8994 - val_loss: 17417.5610\n",
      "Epoch 1281/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14330.6075 - val_loss: 15012.2812\n",
      "Epoch 1282/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15054.2393 - val_loss: 12255.7080\n",
      "Epoch 1283/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12963.7576 - val_loss: 9157.9772\n",
      "Epoch 1284/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 10596.0827 - val_loss: 13897.1290\n",
      "Epoch 1285/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11809.8703 - val_loss: 13408.8487\n",
      "Epoch 1286/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13592.6114 - val_loss: 13639.2675\n",
      "Epoch 1287/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13095.5025 - val_loss: 10466.9354\n",
      "Epoch 1288/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 12967.3309 - val_loss: 10005.0051\n",
      "Epoch 1289/2000\n",
      "2705/2705 [==============================] - 0s 11us/step - loss: 10705.6138 - val_loss: 10242.3034\n",
      "Epoch 1290/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12130.6386 - val_loss: 12209.8273\n",
      "Epoch 1291/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13205.4447 - val_loss: 11607.2958\n",
      "Epoch 1292/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2705/2705 [==============================] - 0s 8us/step - loss: 12903.9011 - val_loss: 14052.4423\n",
      "Epoch 1293/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14961.3287 - val_loss: 6129.0356\n",
      "Epoch 1294/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 9872.0507 - val_loss: 8529.7278\n",
      "Epoch 1295/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 9116.9313 - val_loss: 11262.3346\n",
      "Epoch 1296/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 13040.2639 - val_loss: 12815.6708\n",
      "Epoch 1297/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12842.8908 - val_loss: 11482.6423\n",
      "Epoch 1298/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12425.7891 - val_loss: 13615.0723\n",
      "Epoch 1299/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14480.2549 - val_loss: 10860.6299\n",
      "Epoch 1300/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12144.7903 - val_loss: 14800.5868\n",
      "Epoch 1301/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14165.1589 - val_loss: 11504.9831\n",
      "Epoch 1302/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14887.3476 - val_loss: 10379.0452\n",
      "Epoch 1303/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 9939.8759 - val_loss: 10182.8398\n",
      "Epoch 1304/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 10991.4715 - val_loss: 13468.5186\n",
      "Epoch 1305/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13123.6736 - val_loss: 11744.8664\n",
      "Epoch 1306/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13791.3231 - val_loss: 11841.9865\n",
      "Epoch 1307/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11740.3223 - val_loss: 13369.2311\n",
      "Epoch 1308/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16794.0118 - val_loss: 9202.2599\n",
      "Epoch 1309/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12637.8810 - val_loss: 6696.1861\n",
      "Epoch 1310/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 9021.7719 - val_loss: 11775.5677\n",
      "Epoch 1311/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13856.0244 - val_loss: 8269.5341\n",
      "Epoch 1312/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 13432.9539 - val_loss: 7933.2361\n",
      "Epoch 1313/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 8772.6692 - val_loss: 8699.3947\n",
      "Epoch 1314/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 10199.6883 - val_loss: 12170.2870\n",
      "Epoch 1315/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12996.0270 - val_loss: 7614.5861\n",
      "Epoch 1316/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 8963.3063 - val_loss: 12048.9242\n",
      "Epoch 1317/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12607.9359 - val_loss: 10139.9679\n",
      "Epoch 1318/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13165.9793 - val_loss: 9646.6017\n",
      "Epoch 1319/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10337.5796 - val_loss: 9153.4350\n",
      "Epoch 1320/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 10111.8842 - val_loss: 13606.9361\n",
      "Epoch 1321/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14083.5427 - val_loss: 11002.8432\n",
      "Epoch 1322/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12669.0869 - val_loss: 12511.3208\n",
      "Epoch 1323/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12220.6782 - val_loss: 11201.7527\n",
      "Epoch 1324/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 13543.8162 - val_loss: 10267.5698\n",
      "Epoch 1325/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11356.5720 - val_loss: 9278.3986\n",
      "Epoch 1326/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12588.9987 - val_loss: 10845.7024\n",
      "Epoch 1327/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11049.3747 - val_loss: 9182.2350\n",
      "Epoch 1328/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10606.9918 - val_loss: 13207.0216\n",
      "Epoch 1329/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12751.6636 - val_loss: 11408.3057\n",
      "Epoch 1330/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13372.4337 - val_loss: 10454.6865\n",
      "Epoch 1331/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 10478.8866 - val_loss: 11652.9894\n",
      "Epoch 1332/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14090.4044 - val_loss: 11597.0541\n",
      "Epoch 1333/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13111.1318 - val_loss: 9645.6535\n",
      "Epoch 1334/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11412.6994 - val_loss: 11890.7443\n",
      "Epoch 1335/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11946.1292 - val_loss: 10954.7595\n",
      "Epoch 1336/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13459.6864 - val_loss: 12256.1481\n",
      "Epoch 1337/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12989.8859 - val_loss: 11012.4442\n",
      "Epoch 1338/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13228.9253 - val_loss: 11763.2823\n",
      "Epoch 1339/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15853.7998 - val_loss: 6618.4466\n",
      "Epoch 1340/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 8351.8363 - val_loss: 11641.5580\n",
      "Epoch 1341/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13851.1899 - val_loss: 9396.0244\n",
      "Epoch 1342/2000\n",
      "2705/2705 [==============================] - 0s 11us/step - loss: 12807.9941 - val_loss: 10943.8415\n",
      "Epoch 1343/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 10152.1855 - val_loss: 12786.5800\n",
      "Epoch 1344/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13203.7743 - val_loss: 14796.7691\n",
      "Epoch 1345/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12850.2222 - val_loss: 14666.0369\n",
      "Epoch 1346/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15300.9714 - val_loss: 13456.5067\n",
      "Epoch 1347/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14884.6509 - val_loss: 10983.2330\n",
      "Epoch 1348/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13575.8819 - val_loss: 10151.3843\n",
      "Epoch 1349/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12440.6706 - val_loss: 8917.7154\n",
      "Epoch 1350/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12242.6641 - val_loss: 12981.4876\n",
      "Epoch 1351/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13120.9543 - val_loss: 13346.9533\n",
      "Epoch 1352/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12802.7921 - val_loss: 18714.4722\n",
      "Epoch 1353/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16820.8555 - val_loss: 14755.5766\n",
      "Epoch 1354/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14816.3672 - val_loss: 15142.8024\n",
      "Epoch 1355/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15711.4890 - val_loss: 10460.4092\n",
      "Epoch 1356/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11840.6317 - val_loss: 17269.7788\n",
      "Epoch 1357/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16045.6207 - val_loss: 13952.4838\n",
      "Epoch 1358/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 17921.4346 - val_loss: 13087.0411\n",
      "Epoch 1359/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13664.2368 - val_loss: 13536.2839\n",
      "Epoch 1360/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17652.1905 - val_loss: 12597.3613\n",
      "Epoch 1361/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13593.1491 - val_loss: 14083.6356\n",
      "Epoch 1362/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13390.3265 - val_loss: 21565.9124\n",
      "Epoch 1363/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2705/2705 [==============================] - 0s 9us/step - loss: 20218.9274 - val_loss: 12457.4953\n",
      "Epoch 1364/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13235.9122 - val_loss: 16765.1263\n",
      "Epoch 1365/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 17356.5801 - val_loss: 12146.3187\n",
      "Epoch 1366/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13488.2463 - val_loss: 21071.2199\n",
      "Epoch 1367/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 18299.2323 - val_loss: 14378.9640\n",
      "Epoch 1368/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16239.7229 - val_loss: 14367.9190\n",
      "Epoch 1369/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15640.8574 - val_loss: 9883.4156\n",
      "Epoch 1370/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 13013.9480 - val_loss: 12973.2907\n",
      "Epoch 1371/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12217.4786 - val_loss: 12404.4850\n",
      "Epoch 1372/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14847.8247 - val_loss: 14868.5843\n",
      "Epoch 1373/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15391.5240 - val_loss: 11996.4726\n",
      "Epoch 1374/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11940.1620 - val_loss: 16041.2822\n",
      "Epoch 1375/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16092.8700 - val_loss: 13890.4246\n",
      "Epoch 1376/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15877.8773 - val_loss: 15680.7326\n",
      "Epoch 1377/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 13732.3758 - val_loss: 15992.1019\n",
      "Epoch 1378/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14855.7250 - val_loss: 18319.7022\n",
      "Epoch 1379/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16440.5159 - val_loss: 12574.7292\n",
      "Epoch 1380/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15424.6919 - val_loss: 13116.7128\n",
      "Epoch 1381/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13956.4622 - val_loss: 12297.9837\n",
      "Epoch 1382/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14658.6012 - val_loss: 12581.5696\n",
      "Epoch 1383/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13913.6718 - val_loss: 9948.8612\n",
      "Epoch 1384/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12187.8234 - val_loss: 10724.3618\n",
      "Epoch 1385/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11441.5472 - val_loss: 12162.9948\n",
      "Epoch 1386/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14562.3929 - val_loss: 12145.1823\n",
      "Epoch 1387/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 12067.2719 - val_loss: 14012.2114\n",
      "Epoch 1388/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14529.7970 - val_loss: 17229.8219\n",
      "Epoch 1389/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 18210.0652 - val_loss: 10326.4072\n",
      "Epoch 1390/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12947.6290 - val_loss: 13586.9813\n",
      "Epoch 1391/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13478.5465 - val_loss: 11432.2665\n",
      "Epoch 1392/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12061.1378 - val_loss: 17339.4308\n",
      "Epoch 1393/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15403.7355 - val_loss: 13199.8667\n",
      "Epoch 1394/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14376.1433 - val_loss: 15124.2209\n",
      "Epoch 1395/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15728.5806 - val_loss: 11520.5717\n",
      "Epoch 1396/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14716.6711 - val_loss: 10778.5347\n",
      "Epoch 1397/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12361.3188 - val_loss: 9937.0109\n",
      "Epoch 1398/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12958.5692 - val_loss: 13264.7043\n",
      "Epoch 1399/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 12510.1520 - val_loss: 14275.0162\n",
      "Epoch 1400/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15064.1373 - val_loss: 15087.6522\n",
      "Epoch 1401/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15440.4997 - val_loss: 13583.3237\n",
      "Epoch 1402/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13655.6714 - val_loss: 16568.8211\n",
      "Epoch 1403/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15188.5366 - val_loss: 10884.4553\n",
      "Epoch 1404/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13836.8488 - val_loss: 11801.9992\n",
      "Epoch 1405/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13387.5412 - val_loss: 12082.1084\n",
      "Epoch 1406/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13079.9957 - val_loss: 17181.4145\n",
      "Epoch 1407/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15352.3780 - val_loss: 13210.0060\n",
      "Epoch 1408/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14941.2966 - val_loss: 13945.0112\n",
      "Epoch 1409/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12651.2224 - val_loss: 11902.6810\n",
      "Epoch 1410/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 14746.6269 - val_loss: 12583.7266\n",
      "Epoch 1411/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13523.3520 - val_loss: 10884.9462\n",
      "Epoch 1412/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13122.7660 - val_loss: 13037.9546\n",
      "Epoch 1413/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14883.5513 - val_loss: 8446.6117\n",
      "Epoch 1414/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 12783.2323 - val_loss: 10360.5857\n",
      "Epoch 1415/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11164.0637 - val_loss: 11595.9326\n",
      "Epoch 1416/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 12876.9886 - val_loss: 17102.3473\n",
      "Epoch 1417/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17169.6950 - val_loss: 11472.9633\n",
      "Epoch 1418/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13141.7758 - val_loss: 15009.1779\n",
      "Epoch 1419/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 16277.1883 - val_loss: 10295.4562\n",
      "Epoch 1420/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 11415.8228 - val_loss: 15258.6657\n",
      "Epoch 1421/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13530.6268 - val_loss: 11484.6396\n",
      "Epoch 1422/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13349.8142 - val_loss: 13858.2806\n",
      "Epoch 1423/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13369.3548 - val_loss: 15497.1113\n",
      "Epoch 1424/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 14009.5870 - val_loss: 20653.2589\n",
      "Epoch 1425/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16084.5324 - val_loss: 17502.6569\n",
      "Epoch 1426/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 17273.3247 - val_loss: 16033.7696\n",
      "Epoch 1427/2000\n",
      "2705/2705 [==============================] - 0s 13us/step - loss: 15535.7576 - val_loss: 12476.9236\n",
      "Epoch 1428/2000\n",
      "2705/2705 [==============================] - 0s 12us/step - loss: 15070.6600 - val_loss: 10982.2994\n",
      "Epoch 1429/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12287.0569 - val_loss: 9871.8139\n",
      "Epoch 1430/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12293.2952 - val_loss: 14048.9747\n",
      "Epoch 1431/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 13959.9946 - val_loss: 12468.5548\n",
      "Epoch 1432/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15636.6851 - val_loss: 13105.1290\n",
      "Epoch 1433/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 15598.1373 - val_loss: 9984.8247\n",
      "Epoch 1434/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2705/2705 [==============================] - 0s 9us/step - loss: 13740.4637 - val_loss: 18008.6559\n",
      "Epoch 1435/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16023.7926 - val_loss: 15590.6275\n",
      "Epoch 1436/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17242.3029 - val_loss: 18672.4796\n",
      "Epoch 1437/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 17089.0517 - val_loss: 20054.7659\n",
      "Epoch 1438/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 22353.8756 - val_loss: 19488.0857\n",
      "Epoch 1439/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 19019.0955 - val_loss: 14307.3457\n",
      "Epoch 1440/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16893.2709 - val_loss: 16594.9534\n",
      "Epoch 1441/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 16098.7888 - val_loss: 17252.4374\n",
      "Epoch 1442/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 19542.2446 - val_loss: 18074.0167\n",
      "Epoch 1443/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 19145.0740 - val_loss: 13099.3787\n",
      "Epoch 1444/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17466.7743 - val_loss: 12683.6618\n",
      "Epoch 1445/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13833.9284 - val_loss: 12940.4632\n",
      "Epoch 1446/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16797.5934 - val_loss: 16832.1901\n",
      "Epoch 1447/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18033.2487 - val_loss: 18023.8269\n",
      "Epoch 1448/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 19946.2149 - val_loss: 17843.4465\n",
      "Epoch 1449/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 20869.0906 - val_loss: 9636.0131\n",
      "Epoch 1450/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11874.6932 - val_loss: 16842.5906\n",
      "Epoch 1451/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16415.8767 - val_loss: 14262.1711\n",
      "Epoch 1452/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 18396.1352 - val_loss: 13149.8057\n",
      "Epoch 1453/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13613.7584 - val_loss: 13505.5283\n",
      "Epoch 1454/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14604.8015 - val_loss: 16300.5432\n",
      "Epoch 1455/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15616.4925 - val_loss: 13860.6623\n",
      "Epoch 1456/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15334.0636 - val_loss: 15214.3140\n",
      "Epoch 1457/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14908.5947 - val_loss: 13583.4732\n",
      "Epoch 1458/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14585.3720 - val_loss: 16013.4651\n",
      "Epoch 1459/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16226.6101 - val_loss: 14740.7052\n",
      "Epoch 1460/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14859.3982 - val_loss: 17791.4156\n",
      "Epoch 1461/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17541.5141 - val_loss: 15631.4623\n",
      "Epoch 1462/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16432.5138 - val_loss: 17025.5618\n",
      "Epoch 1463/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15416.8299 - val_loss: 13273.4460\n",
      "Epoch 1464/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16109.5794 - val_loss: 11867.9026\n",
      "Epoch 1465/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12281.0173 - val_loss: 12570.8152\n",
      "Epoch 1466/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13031.1439 - val_loss: 15249.4693\n",
      "Epoch 1467/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 17022.9823 - val_loss: 7850.1210\n",
      "Epoch 1468/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 8879.4176 - val_loss: 13180.0299\n",
      "Epoch 1469/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13082.8016 - val_loss: 13445.6901\n",
      "Epoch 1470/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14389.3287 - val_loss: 14456.1690\n",
      "Epoch 1471/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 17137.6846 - val_loss: 7664.3972\n",
      "Epoch 1472/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 9969.1593 - val_loss: 11165.6313\n",
      "Epoch 1473/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10557.4013 - val_loss: 14797.1284\n",
      "Epoch 1474/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15100.3951 - val_loss: 13519.8960\n",
      "Epoch 1475/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12556.3301 - val_loss: 12441.6700\n",
      "Epoch 1476/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13383.4717 - val_loss: 15142.9662\n",
      "Epoch 1477/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13847.2750 - val_loss: 11794.3372\n",
      "Epoch 1478/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14336.0775 - val_loss: 11774.9579\n",
      "Epoch 1479/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11906.2776 - val_loss: 12131.0721\n",
      "Epoch 1480/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15341.8761 - val_loss: 10189.3026\n",
      "Epoch 1481/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10972.6378 - val_loss: 11919.9006\n",
      "Epoch 1482/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14247.4470 - val_loss: 12629.5665\n",
      "Epoch 1483/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11584.7987 - val_loss: 13164.8912\n",
      "Epoch 1484/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15099.6461 - val_loss: 11482.6716\n",
      "Epoch 1485/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13354.8541 - val_loss: 9619.9910\n",
      "Epoch 1486/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12452.5988 - val_loss: 10991.2952\n",
      "Epoch 1487/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10950.5739 - val_loss: 11263.2654\n",
      "Epoch 1488/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 11905.0208 - val_loss: 14420.8879\n",
      "Epoch 1489/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 13866.0880 - val_loss: 11219.1727\n",
      "Epoch 1490/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11245.7196 - val_loss: 16888.2812\n",
      "Epoch 1491/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14957.7904 - val_loss: 12182.9245\n",
      "Epoch 1492/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11843.6026 - val_loss: 15099.8482\n",
      "Epoch 1493/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14044.2148 - val_loss: 9842.3111\n",
      "Epoch 1494/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11870.1026 - val_loss: 13737.8617\n",
      "Epoch 1495/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11960.0447 - val_loss: 14235.2243\n",
      "Epoch 1496/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13754.3399 - val_loss: 14497.6824\n",
      "Epoch 1497/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13408.0309 - val_loss: 8363.6618\n",
      "Epoch 1498/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12264.5517 - val_loss: 8797.0360\n",
      "Epoch 1499/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 10864.9172 - val_loss: 8283.4252\n",
      "Epoch 1500/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 10292.4624 - val_loss: 11419.7655\n",
      "Epoch 1501/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12827.5778 - val_loss: 10650.7658\n",
      "Epoch 1502/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12952.9770 - val_loss: 11008.7420\n",
      "Epoch 1503/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11600.9917 - val_loss: 10371.7899\n",
      "Epoch 1504/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12423.3154 - val_loss: 13016.4986\n",
      "Epoch 1505/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2705/2705 [==============================] - 0s 7us/step - loss: 13986.1703 - val_loss: 8889.4142\n",
      "Epoch 1506/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 9736.4338 - val_loss: 15919.8703\n",
      "Epoch 1507/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15105.5612 - val_loss: 12401.7168\n",
      "Epoch 1508/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13518.9072 - val_loss: 11783.7236\n",
      "Epoch 1509/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 11945.3092 - val_loss: 11088.8502\n",
      "Epoch 1510/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12147.3848 - val_loss: 11306.2375\n",
      "Epoch 1511/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12754.3160 - val_loss: 10919.3051\n",
      "Epoch 1512/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13327.9925 - val_loss: 12052.5311\n",
      "Epoch 1513/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 13127.4323 - val_loss: 12843.5161\n",
      "Epoch 1514/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14809.7611 - val_loss: 14821.2607\n",
      "Epoch 1515/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16660.1247 - val_loss: 8761.3689\n",
      "Epoch 1516/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12753.0794 - val_loss: 8956.9503\n",
      "Epoch 1517/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 9241.6521 - val_loss: 11493.0718\n",
      "Epoch 1518/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14292.0013 - val_loss: 12228.5913\n",
      "Epoch 1519/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12593.9563 - val_loss: 11844.5316\n",
      "Epoch 1520/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13407.9330 - val_loss: 14307.5754\n",
      "Epoch 1521/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16427.0350 - val_loss: 11784.4138\n",
      "Epoch 1522/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14777.0488 - val_loss: 14763.4844\n",
      "Epoch 1523/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 17012.6214 - val_loss: 9429.9653\n",
      "Epoch 1524/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12372.1095 - val_loss: 14226.9957\n",
      "Epoch 1525/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13939.2986 - val_loss: 13824.8050\n",
      "Epoch 1526/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15795.9319 - val_loss: 17089.9008\n",
      "Epoch 1527/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 19071.2101 - val_loss: 12690.5791\n",
      "Epoch 1528/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16703.2640 - val_loss: 18770.9994\n",
      "Epoch 1529/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 20229.4897 - val_loss: 18163.5005\n",
      "Epoch 1530/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 20180.1196 - val_loss: 26524.4081\n",
      "Epoch 1531/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 24362.6942 - val_loss: 19872.7450\n",
      "Epoch 1532/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 21620.7259 - val_loss: 15802.9792\n",
      "Epoch 1533/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 17036.9161 - val_loss: 13072.2836\n",
      "Epoch 1534/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 15427.7687 - val_loss: 18236.7146\n",
      "Epoch 1535/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 21413.6166 - val_loss: 10432.2650\n",
      "Epoch 1536/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11477.6262 - val_loss: 14445.8025\n",
      "Epoch 1537/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12542.1413 - val_loss: 16378.3836\n",
      "Epoch 1538/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14099.8962 - val_loss: 19981.9377\n",
      "Epoch 1539/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16860.0124 - val_loss: 11197.9174\n",
      "Epoch 1540/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14046.3801 - val_loss: 12387.3685\n",
      "Epoch 1541/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11884.1274 - val_loss: 11539.5053\n",
      "Epoch 1542/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13883.7100 - val_loss: 10437.0066\n",
      "Epoch 1543/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11583.2180 - val_loss: 11004.9831\n",
      "Epoch 1544/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12896.7571 - val_loss: 13479.5914\n",
      "Epoch 1545/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14346.5382 - val_loss: 10440.7186\n",
      "Epoch 1546/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13053.9853 - val_loss: 12329.9981\n",
      "Epoch 1547/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14512.7390 - val_loss: 8527.1954\n",
      "Epoch 1548/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 10306.0399 - val_loss: 12959.7401\n",
      "Epoch 1549/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 15330.7341 - val_loss: 8087.1146\n",
      "Epoch 1550/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11270.2713 - val_loss: 9485.7936\n",
      "Epoch 1551/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 11823.0752 - val_loss: 10975.3619\n",
      "Epoch 1552/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12885.9004 - val_loss: 12546.0990\n",
      "Epoch 1553/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14450.8703 - val_loss: 9640.5373\n",
      "Epoch 1554/2000\n",
      "2705/2705 [==============================] - 0s 11us/step - loss: 11529.3275 - val_loss: 12784.9925\n",
      "Epoch 1555/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13437.9252 - val_loss: 11008.5836\n",
      "Epoch 1556/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13542.6686 - val_loss: 9643.9962\n",
      "Epoch 1557/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11289.1440 - val_loss: 7888.0190\n",
      "Epoch 1558/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 10192.7999 - val_loss: 10875.5122\n",
      "Epoch 1559/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12251.0373 - val_loss: 10217.2803\n",
      "Epoch 1560/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11828.8064 - val_loss: 12801.3017\n",
      "Epoch 1561/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12727.4352 - val_loss: 11313.4881\n",
      "Epoch 1562/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13558.9062 - val_loss: 11559.3120\n",
      "Epoch 1563/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11862.0769 - val_loss: 11836.1966\n",
      "Epoch 1564/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13007.9872 - val_loss: 12500.4500\n",
      "Epoch 1565/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12738.2963 - val_loss: 12512.7828\n",
      "Epoch 1566/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15021.3112 - val_loss: 12058.3851\n",
      "Epoch 1567/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14604.3798 - val_loss: 9168.4860\n",
      "Epoch 1568/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 11303.7425 - val_loss: 13258.2880\n",
      "Epoch 1569/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11763.6860 - val_loss: 11235.3543\n",
      "Epoch 1570/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12525.3386 - val_loss: 14552.3363\n",
      "Epoch 1571/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14866.2552 - val_loss: 11657.8274\n",
      "Epoch 1572/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14805.6121 - val_loss: 10092.0134\n",
      "Epoch 1573/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11076.9284 - val_loss: 10728.0263\n",
      "Epoch 1574/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11436.5724 - val_loss: 12530.5649\n",
      "Epoch 1575/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 12152.1723 - val_loss: 11818.7869\n",
      "Epoch 1576/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2705/2705 [==============================] - 0s 9us/step - loss: 12656.4790 - val_loss: 12594.2398\n",
      "Epoch 1577/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13327.5458 - val_loss: 9204.7763\n",
      "Epoch 1578/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12030.3727 - val_loss: 10983.7529\n",
      "Epoch 1579/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10605.2494 - val_loss: 11567.6657\n",
      "Epoch 1580/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12716.3091 - val_loss: 11762.5770\n",
      "Epoch 1581/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13270.8886 - val_loss: 9876.4495\n",
      "Epoch 1582/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 13229.4752 - val_loss: 9052.6127\n",
      "Epoch 1583/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10099.7088 - val_loss: 9759.7235\n",
      "Epoch 1584/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12563.7688 - val_loss: 10982.0326\n",
      "Epoch 1585/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11038.7558 - val_loss: 11476.6395\n",
      "Epoch 1586/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11670.7653 - val_loss: 15055.3085\n",
      "Epoch 1587/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13655.6764 - val_loss: 11571.7728\n",
      "Epoch 1588/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12731.5644 - val_loss: 11934.1251\n",
      "Epoch 1589/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11139.8883 - val_loss: 11024.4329\n",
      "Epoch 1590/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13032.4249 - val_loss: 10302.0851\n",
      "Epoch 1591/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12604.7744 - val_loss: 9508.1631\n",
      "Epoch 1592/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12496.0045 - val_loss: 9955.9697\n",
      "Epoch 1593/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 8538.3612 - val_loss: 12827.9904\n",
      "Epoch 1594/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 12890.5335 - val_loss: 14322.0677\n",
      "Epoch 1595/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 13661.0250 - val_loss: 9269.9969\n",
      "Epoch 1596/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 11768.1242 - val_loss: 11431.2510\n",
      "Epoch 1597/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11689.9162 - val_loss: 13348.7712\n",
      "Epoch 1598/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15808.5519 - val_loss: 10022.6019\n",
      "Epoch 1599/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10120.8351 - val_loss: 11132.4060\n",
      "Epoch 1600/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12556.7372 - val_loss: 13671.0210\n",
      "Epoch 1601/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13886.4930 - val_loss: 12044.9363\n",
      "Epoch 1602/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12767.7756 - val_loss: 16614.9864\n",
      "Epoch 1603/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15023.9217 - val_loss: 15120.0107\n",
      "Epoch 1604/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13411.3873 - val_loss: 21001.9419\n",
      "Epoch 1605/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 18525.3337 - val_loss: 12391.3791\n",
      "Epoch 1606/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14916.0036 - val_loss: 13018.2826\n",
      "Epoch 1607/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14749.5582 - val_loss: 10333.5774\n",
      "Epoch 1608/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11588.4132 - val_loss: 13283.2156\n",
      "Epoch 1609/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12514.9446 - val_loss: 14029.1616\n",
      "Epoch 1610/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15288.1947 - val_loss: 15705.3563\n",
      "Epoch 1611/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 14108.7420 - val_loss: 14542.6966\n",
      "Epoch 1612/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14666.1086 - val_loss: 15525.7587\n",
      "Epoch 1613/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14020.4465 - val_loss: 14680.5112\n",
      "Epoch 1614/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14811.6519 - val_loss: 14779.2381\n",
      "Epoch 1615/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14271.5431 - val_loss: 13030.7088\n",
      "Epoch 1616/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15711.2622 - val_loss: 13474.1871\n",
      "Epoch 1617/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11673.1311 - val_loss: 15104.8160\n",
      "Epoch 1618/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15038.2497 - val_loss: 16110.2444\n",
      "Epoch 1619/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16171.4931 - val_loss: 9969.2033\n",
      "Epoch 1620/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12863.1714 - val_loss: 11377.0752\n",
      "Epoch 1621/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11828.5152 - val_loss: 11396.1942\n",
      "Epoch 1622/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15179.3308 - val_loss: 10119.9855\n",
      "Epoch 1623/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12686.0456 - val_loss: 9382.2585\n",
      "Epoch 1624/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 11567.3980 - val_loss: 12215.7479\n",
      "Epoch 1625/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12328.2737 - val_loss: 11737.4273\n",
      "Epoch 1626/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13001.5912 - val_loss: 14375.3891\n",
      "Epoch 1627/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14165.8037 - val_loss: 11331.8043\n",
      "Epoch 1628/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13159.6917 - val_loss: 11369.6508\n",
      "Epoch 1629/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12922.8259 - val_loss: 10043.2668\n",
      "Epoch 1630/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14462.3341 - val_loss: 10849.4716\n",
      "Epoch 1631/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11274.0425 - val_loss: 12133.3895\n",
      "Epoch 1632/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14853.1567 - val_loss: 10488.1497\n",
      "Epoch 1633/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 11372.2217 - val_loss: 9489.6636\n",
      "Epoch 1634/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12187.2642 - val_loss: 13091.4184\n",
      "Epoch 1635/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14033.9638 - val_loss: 9625.5570\n",
      "Epoch 1636/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 10436.0957 - val_loss: 11301.6160\n",
      "Epoch 1637/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11236.6313 - val_loss: 13196.3507\n",
      "Epoch 1638/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14030.7734 - val_loss: 12545.6715\n",
      "Epoch 1639/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12718.5709 - val_loss: 10092.0306\n",
      "Epoch 1640/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10252.9117 - val_loss: 15105.6445\n",
      "Epoch 1641/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12902.7030 - val_loss: 12912.7170\n",
      "Epoch 1642/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12389.7671 - val_loss: 14364.9844\n",
      "Epoch 1643/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14641.6732 - val_loss: 8521.5054\n",
      "Epoch 1644/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 9704.8575 - val_loss: 14050.8140\n",
      "Epoch 1645/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14314.8680 - val_loss: 9700.8126\n",
      "Epoch 1646/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 11840.8458 - val_loss: 10464.6967\n",
      "Epoch 1647/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2705/2705 [==============================] - 0s 8us/step - loss: 11911.5955 - val_loss: 10330.4686\n",
      "Epoch 1648/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12295.8618 - val_loss: 11604.4083\n",
      "Epoch 1649/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 11561.0928 - val_loss: 10419.3478\n",
      "Epoch 1650/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11474.7021 - val_loss: 13960.1547\n",
      "Epoch 1651/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15941.4914 - val_loss: 8128.5457\n",
      "Epoch 1652/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11328.7730 - val_loss: 8994.7063\n",
      "Epoch 1653/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 9186.2086 - val_loss: 10157.7258\n",
      "Epoch 1654/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 10898.7796 - val_loss: 13787.8728\n",
      "Epoch 1655/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14677.3305 - val_loss: 7957.6904\n",
      "Epoch 1656/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10905.4452 - val_loss: 9858.0452\n",
      "Epoch 1657/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 10223.5104 - val_loss: 10503.6002\n",
      "Epoch 1658/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11563.6033 - val_loss: 12066.9661\n",
      "Epoch 1659/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12211.7292 - val_loss: 11198.4380\n",
      "Epoch 1660/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11993.3128 - val_loss: 13389.1074\n",
      "Epoch 1661/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12544.1710 - val_loss: 10301.3665\n",
      "Epoch 1662/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12549.3638 - val_loss: 12577.8781\n",
      "Epoch 1663/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14531.3034 - val_loss: 8015.6235\n",
      "Epoch 1664/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 8660.3798 - val_loss: 13251.3421\n",
      "Epoch 1665/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12618.5124 - val_loss: 11550.1502\n",
      "Epoch 1666/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12974.5216 - val_loss: 10664.5816\n",
      "Epoch 1667/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 9015.9854 - val_loss: 14095.7089\n",
      "Epoch 1668/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13075.0386 - val_loss: 13364.3721\n",
      "Epoch 1669/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12209.2489 - val_loss: 10598.5927\n",
      "Epoch 1670/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 9415.9400 - val_loss: 14423.3782\n",
      "Epoch 1671/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 13413.7528 - val_loss: 9633.3194\n",
      "Epoch 1672/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 10344.3464 - val_loss: 12229.5107\n",
      "Epoch 1673/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 12393.1176 - val_loss: 9887.4866\n",
      "Epoch 1674/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11713.4651 - val_loss: 10911.7104\n",
      "Epoch 1675/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 9800.0040 - val_loss: 12354.7811\n",
      "Epoch 1676/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14151.2853 - val_loss: 8722.7907\n",
      "Epoch 1677/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 9573.5019 - val_loss: 7469.2847\n",
      "Epoch 1678/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 8497.7487 - val_loss: 9952.2257\n",
      "Epoch 1679/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10359.0981 - val_loss: 9513.1350\n",
      "Epoch 1680/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10611.6761 - val_loss: 12881.2109\n",
      "Epoch 1681/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 14432.1853 - val_loss: 12806.8133\n",
      "Epoch 1682/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14233.5869 - val_loss: 15050.9284\n",
      "Epoch 1683/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16113.3518 - val_loss: 11188.8092\n",
      "Epoch 1684/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13855.6011 - val_loss: 12636.9418\n",
      "Epoch 1685/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15147.8537 - val_loss: 11149.6674\n",
      "Epoch 1686/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12704.6865 - val_loss: 18433.4902\n",
      "Epoch 1687/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16627.8537 - val_loss: 16612.9527\n",
      "Epoch 1688/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 16546.8236 - val_loss: 15955.7032\n",
      "Epoch 1689/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16077.8574 - val_loss: 12059.7475\n",
      "Epoch 1690/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16182.2415 - val_loss: 9955.1432\n",
      "Epoch 1691/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11193.2224 - val_loss: 10963.1557\n",
      "Epoch 1692/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13295.0736 - val_loss: 11326.4467\n",
      "Epoch 1693/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10379.4627 - val_loss: 14778.1501\n",
      "Epoch 1694/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14589.4935 - val_loss: 14070.6387\n",
      "Epoch 1695/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12276.4653 - val_loss: 14123.8575\n",
      "Epoch 1696/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14068.0644 - val_loss: 16030.9388\n",
      "Epoch 1697/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 14762.2527 - val_loss: 11444.1574\n",
      "Epoch 1698/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13198.4204 - val_loss: 13844.9334\n",
      "Epoch 1699/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15857.2485 - val_loss: 6916.6943\n",
      "Epoch 1700/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 8599.3609 - val_loss: 10373.2555\n",
      "Epoch 1701/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 10717.6994 - val_loss: 10644.5297\n",
      "Epoch 1702/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 13117.4525 - val_loss: 9827.2789\n",
      "Epoch 1703/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10838.3014 - val_loss: 8861.8341\n",
      "Epoch 1704/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10939.7664 - val_loss: 10508.9894\n",
      "Epoch 1705/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11456.9125 - val_loss: 9534.0591\n",
      "Epoch 1706/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11343.1419 - val_loss: 9999.8952\n",
      "Epoch 1707/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12031.4494 - val_loss: 9509.0634\n",
      "Epoch 1708/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 10255.4268 - val_loss: 12486.4989\n",
      "Epoch 1709/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11188.0752 - val_loss: 11281.4553\n",
      "Epoch 1710/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14115.7360 - val_loss: 8964.1264\n",
      "Epoch 1711/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10461.9149 - val_loss: 7192.1042\n",
      "Epoch 1712/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 8639.8168 - val_loss: 10730.7274\n",
      "Epoch 1713/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11078.4227 - val_loss: 9178.3034\n",
      "Epoch 1714/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10355.8029 - val_loss: 10223.9011\n",
      "Epoch 1715/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 11197.3345 - val_loss: 7853.3963\n",
      "Epoch 1716/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 9579.4727 - val_loss: 9725.3356\n",
      "Epoch 1717/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 10221.4038 - val_loss: 9757.0775\n",
      "Epoch 1718/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2705/2705 [==============================] - 0s 8us/step - loss: 11297.3579 - val_loss: 10276.5945\n",
      "Epoch 1719/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11464.2599 - val_loss: 7715.0681\n",
      "Epoch 1720/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 9750.3288 - val_loss: 11448.5131\n",
      "Epoch 1721/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11012.9969 - val_loss: 11716.5929\n",
      "Epoch 1722/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13527.7164 - val_loss: 10136.3542\n",
      "Epoch 1723/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 9950.6093 - val_loss: 9722.8535\n",
      "Epoch 1724/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12183.9504 - val_loss: 8791.7541\n",
      "Epoch 1725/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 9072.2600 - val_loss: 10263.1236\n",
      "Epoch 1726/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 9547.7113 - val_loss: 16718.9556\n",
      "Epoch 1727/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12497.4909 - val_loss: 15251.0444\n",
      "Epoch 1728/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13517.0574 - val_loss: 15513.7874\n",
      "Epoch 1729/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13266.2811 - val_loss: 11872.0689\n",
      "Epoch 1730/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11310.6738 - val_loss: 12004.6099\n",
      "Epoch 1731/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 11404.4029 - val_loss: 11134.9446\n",
      "Epoch 1732/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12639.0883 - val_loss: 9886.5111\n",
      "Epoch 1733/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 11334.4915 - val_loss: 9046.7035\n",
      "Epoch 1734/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12165.8398 - val_loss: 9951.0878\n",
      "Epoch 1735/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 9602.6367 - val_loss: 10747.9731\n",
      "Epoch 1736/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13045.3218 - val_loss: 9210.9127\n",
      "Epoch 1737/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 11179.5953 - val_loss: 9125.1131\n",
      "Epoch 1738/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10883.3271 - val_loss: 11108.6081\n",
      "Epoch 1739/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 13023.9677 - val_loss: 8346.4021\n",
      "Epoch 1740/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10017.9055 - val_loss: 11910.6069\n",
      "Epoch 1741/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 10088.1512 - val_loss: 11917.1620\n",
      "Epoch 1742/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13421.8497 - val_loss: 10844.7912\n",
      "Epoch 1743/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10735.8893 - val_loss: 11756.3661\n",
      "Epoch 1744/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12284.4741 - val_loss: 14482.5676\n",
      "Epoch 1745/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 14072.1079 - val_loss: 10681.6617\n",
      "Epoch 1746/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11303.5932 - val_loss: 12389.3962\n",
      "Epoch 1747/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12542.7969 - val_loss: 10217.0287\n",
      "Epoch 1748/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 12084.0023 - val_loss: 10732.4412\n",
      "Epoch 1749/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13779.7147 - val_loss: 7460.6767\n",
      "Epoch 1750/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 7575.7925 - val_loss: 13718.2624\n",
      "Epoch 1751/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13219.2137 - val_loss: 8652.4384\n",
      "Epoch 1752/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 9294.0808 - val_loss: 12953.9153\n",
      "Epoch 1753/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12060.3447 - val_loss: 9214.8177\n",
      "Epoch 1754/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 11665.4534 - val_loss: 8540.1554\n",
      "Epoch 1755/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 8723.3074 - val_loss: 9786.6475\n",
      "Epoch 1756/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12935.4334 - val_loss: 7468.5881\n",
      "Epoch 1757/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 8426.4969 - val_loss: 8506.7587\n",
      "Epoch 1758/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12145.9141 - val_loss: 7223.5547\n",
      "Epoch 1759/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 7950.9050 - val_loss: 8670.8269\n",
      "Epoch 1760/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 9980.0149 - val_loss: 10322.4138\n",
      "Epoch 1761/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 9145.4378 - val_loss: 9695.7484\n",
      "Epoch 1762/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11790.4017 - val_loss: 9066.2555\n",
      "Epoch 1763/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 9223.8165 - val_loss: 8280.0938\n",
      "Epoch 1764/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10314.1417 - val_loss: 8622.4498\n",
      "Epoch 1765/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 9832.6346 - val_loss: 8066.4828\n",
      "Epoch 1766/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 9153.6914 - val_loss: 10590.5245\n",
      "Epoch 1767/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 10707.1997 - val_loss: 9590.1820\n",
      "Epoch 1768/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11611.4854 - val_loss: 9034.6816\n",
      "Epoch 1769/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 9787.1515 - val_loss: 9926.6851\n",
      "Epoch 1770/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10628.7890 - val_loss: 12125.0050\n",
      "Epoch 1771/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11774.5121 - val_loss: 11861.8648\n",
      "Epoch 1772/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12256.7456 - val_loss: 10593.7961\n",
      "Epoch 1773/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10773.6957 - val_loss: 9056.4898\n",
      "Epoch 1774/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 9946.0451 - val_loss: 9546.6898\n",
      "Epoch 1775/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 10648.9174 - val_loss: 9277.2290\n",
      "Epoch 1776/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 9581.6882 - val_loss: 11895.6029\n",
      "Epoch 1777/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12860.3708 - val_loss: 8397.2897\n",
      "Epoch 1778/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11331.8808 - val_loss: 10106.2944\n",
      "Epoch 1779/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10591.5923 - val_loss: 10207.1350\n",
      "Epoch 1780/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12857.5892 - val_loss: 10236.2130\n",
      "Epoch 1781/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 11100.4511 - val_loss: 9605.2856\n",
      "Epoch 1782/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11361.7309 - val_loss: 11835.6749\n",
      "Epoch 1783/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14176.5047 - val_loss: 9001.6732\n",
      "Epoch 1784/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12190.4806 - val_loss: 12534.4607\n",
      "Epoch 1785/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12394.9460 - val_loss: 17194.7215\n",
      "Epoch 1786/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16978.6533 - val_loss: 20604.6315\n",
      "Epoch 1787/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 18293.0429 - val_loss: 14090.0862\n",
      "Epoch 1788/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15705.5323 - val_loss: 13360.3953\n",
      "Epoch 1789/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2705/2705 [==============================] - 0s 8us/step - loss: 16050.9403 - val_loss: 7877.9576\n",
      "Epoch 1790/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 9884.0242 - val_loss: 13439.6468\n",
      "Epoch 1791/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13585.4287 - val_loss: 15491.8043\n",
      "Epoch 1792/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 19157.2171 - val_loss: 11007.7087\n",
      "Epoch 1793/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12437.8157 - val_loss: 10797.2188\n",
      "Epoch 1794/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 12063.8752 - val_loss: 13311.7717\n",
      "Epoch 1795/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 14104.2309 - val_loss: 9661.4973\n",
      "Epoch 1796/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12430.3993 - val_loss: 11206.7455\n",
      "Epoch 1797/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11291.8904 - val_loss: 14295.8820\n",
      "Epoch 1798/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 14048.6555 - val_loss: 14201.0616\n",
      "Epoch 1799/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14833.3567 - val_loss: 10189.7852\n",
      "Epoch 1800/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13577.8162 - val_loss: 11091.4558\n",
      "Epoch 1801/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10792.0691 - val_loss: 11113.5099\n",
      "Epoch 1802/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12300.9186 - val_loss: 12919.5253\n",
      "Epoch 1803/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 10900.5248 - val_loss: 11558.8624\n",
      "Epoch 1804/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 13896.2720 - val_loss: 9349.4834\n",
      "Epoch 1805/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 9452.8434 - val_loss: 9895.3308\n",
      "Epoch 1806/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 10097.9750 - val_loss: 12865.1187\n",
      "Epoch 1807/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12744.4058 - val_loss: 8022.5349\n",
      "Epoch 1808/2000\n",
      "2705/2705 [==============================] - 0s 12us/step - loss: 10067.9482 - val_loss: 11302.9306\n",
      "Epoch 1809/2000\n",
      "2705/2705 [==============================] - 0s 13us/step - loss: 11801.5299 - val_loss: 9249.7762\n",
      "Epoch 1810/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 9951.8383 - val_loss: 13189.2542\n",
      "Epoch 1811/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 11700.5616 - val_loss: 9219.8202\n",
      "Epoch 1812/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 11268.9658 - val_loss: 9040.2835\n",
      "Epoch 1813/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 9909.3264 - val_loss: 9739.0903\n",
      "Epoch 1814/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 11315.8938 - val_loss: 9879.9523\n",
      "Epoch 1815/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 10502.3820 - val_loss: 8933.8763\n",
      "Epoch 1816/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 10745.5344 - val_loss: 9714.3154\n",
      "Epoch 1817/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10898.5248 - val_loss: 8449.3806\n",
      "Epoch 1818/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10807.7190 - val_loss: 9898.3554\n",
      "Epoch 1819/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10770.7647 - val_loss: 10419.0046\n",
      "Epoch 1820/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11941.6606 - val_loss: 9832.0230\n",
      "Epoch 1821/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11316.5328 - val_loss: 9063.4222\n",
      "Epoch 1822/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 12602.1232 - val_loss: 10250.4732\n",
      "Epoch 1823/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 10437.0849 - val_loss: 10539.0473\n",
      "Epoch 1824/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10311.2123 - val_loss: 14976.8835\n",
      "Epoch 1825/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 12816.7382 - val_loss: 11143.8948\n",
      "Epoch 1826/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 12164.0404 - val_loss: 13532.3422\n",
      "Epoch 1827/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 11643.1333 - val_loss: 11375.3636\n",
      "Epoch 1828/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 13305.1265 - val_loss: 10176.0968\n",
      "Epoch 1829/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 10977.1376 - val_loss: 8462.1174\n",
      "Epoch 1830/2000\n",
      "2705/2705 [==============================] - 0s 6us/step - loss: 10133.6614 - val_loss: 11437.4873\n",
      "Epoch 1831/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 12501.2622 - val_loss: 7546.3970\n",
      "Epoch 1832/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 10164.7110 - val_loss: 8450.7734\n",
      "Epoch 1833/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 10111.4296 - val_loss: 8503.5482\n",
      "Epoch 1834/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 9738.2557 - val_loss: 11595.9255\n",
      "Epoch 1835/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12073.5481 - val_loss: 10099.3244\n",
      "Epoch 1836/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 12277.2261 - val_loss: 10666.3526\n",
      "Epoch 1837/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 9826.1580 - val_loss: 10874.2158\n",
      "Epoch 1838/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11794.1778 - val_loss: 10935.7839\n",
      "Epoch 1839/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11679.3265 - val_loss: 9672.6770\n",
      "Epoch 1840/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12842.9165 - val_loss: 6394.7909\n",
      "Epoch 1841/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 7820.3470 - val_loss: 8311.7086\n",
      "Epoch 1842/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 9268.6735 - val_loss: 11252.1748\n",
      "Epoch 1843/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 11863.9115 - val_loss: 8640.0246\n",
      "Epoch 1844/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11165.9554 - val_loss: 8641.2827\n",
      "Epoch 1845/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 9117.8753 - val_loss: 10008.4712\n",
      "Epoch 1846/2000\n",
      "2705/2705 [==============================] - 0s 11us/step - loss: 12324.5309 - val_loss: 8752.3646\n",
      "Epoch 1847/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11619.5188 - val_loss: 6493.0468\n",
      "Epoch 1848/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 7826.8976 - val_loss: 10966.4685\n",
      "Epoch 1849/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 11040.2826 - val_loss: 7718.8567\n",
      "Epoch 1850/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 9006.5546 - val_loss: 11351.3439\n",
      "Epoch 1851/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 12589.5289 - val_loss: 8441.4738\n",
      "Epoch 1852/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 9892.0148 - val_loss: 11960.2668\n",
      "Epoch 1853/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 10934.7297 - val_loss: 10955.5355\n",
      "Epoch 1854/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12971.5322 - val_loss: 12671.9028\n",
      "Epoch 1855/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12407.0784 - val_loss: 11133.5948\n",
      "Epoch 1856/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 13734.6547 - val_loss: 12604.6734\n",
      "Epoch 1857/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15794.1221 - val_loss: 10260.5675\n",
      "Epoch 1858/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 15359.1458 - val_loss: 11973.1206\n",
      "Epoch 1859/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 16011.7676 - val_loss: 10683.0746\n",
      "Epoch 1860/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2705/2705 [==============================] - 0s 8us/step - loss: 13350.7667 - val_loss: 14379.8202\n",
      "Epoch 1861/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14866.8529 - val_loss: 13013.6907\n",
      "Epoch 1862/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 17032.5166 - val_loss: 12951.1344\n",
      "Epoch 1863/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15634.7140 - val_loss: 10106.7336\n",
      "Epoch 1864/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 12851.5594 - val_loss: 11749.5710\n",
      "Epoch 1865/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13189.9057 - val_loss: 8526.7781\n",
      "Epoch 1866/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 9957.1939 - val_loss: 11724.8526\n",
      "Epoch 1867/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12903.2141 - val_loss: 11238.2313\n",
      "Epoch 1868/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13829.8364 - val_loss: 10340.7168\n",
      "Epoch 1869/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12092.2723 - val_loss: 8244.6442\n",
      "Epoch 1870/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11257.5711 - val_loss: 11459.9557\n",
      "Epoch 1871/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10525.4118 - val_loss: 13362.1913\n",
      "Epoch 1872/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11991.9893 - val_loss: 15603.3014\n",
      "Epoch 1873/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14535.3886 - val_loss: 12086.6119\n",
      "Epoch 1874/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10440.7532 - val_loss: 18716.8875\n",
      "Epoch 1875/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 15800.9241 - val_loss: 12253.9010\n",
      "Epoch 1876/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13515.3018 - val_loss: 10638.2340\n",
      "Epoch 1877/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 12019.3863 - val_loss: 10107.8130\n",
      "Epoch 1878/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12178.4411 - val_loss: 9746.9851\n",
      "Epoch 1879/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 10087.3587 - val_loss: 9239.8940\n",
      "Epoch 1880/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 8803.8478 - val_loss: 14436.4234\n",
      "Epoch 1881/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 13694.7627 - val_loss: 9675.1949\n",
      "Epoch 1882/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10878.3820 - val_loss: 9147.5445\n",
      "Epoch 1883/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 8966.6123 - val_loss: 9717.4683\n",
      "Epoch 1884/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 9491.5891 - val_loss: 14624.2513\n",
      "Epoch 1885/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14023.9093 - val_loss: 8493.8853\n",
      "Epoch 1886/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 9100.0465 - val_loss: 12602.2593\n",
      "Epoch 1887/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11315.6922 - val_loss: 9731.0907\n",
      "Epoch 1888/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 9509.5713 - val_loss: 12097.3281\n",
      "Epoch 1889/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 11899.3695 - val_loss: 8291.9689\n",
      "Epoch 1890/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10219.7514 - val_loss: 9106.2006\n",
      "Epoch 1891/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10744.6850 - val_loss: 7812.1748\n",
      "Epoch 1892/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 9126.9221 - val_loss: 9418.5140\n",
      "Epoch 1893/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11133.9354 - val_loss: 7012.2726\n",
      "Epoch 1894/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 9148.7927 - val_loss: 9104.6122\n",
      "Epoch 1895/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 9393.0689 - val_loss: 9462.4025\n",
      "Epoch 1896/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 10145.4477 - val_loss: 10425.7023\n",
      "Epoch 1897/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 10294.7697 - val_loss: 9500.7510\n",
      "Epoch 1898/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10004.0869 - val_loss: 12196.6005\n",
      "Epoch 1899/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11639.1838 - val_loss: 9600.0408\n",
      "Epoch 1900/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 11708.7763 - val_loss: 8802.1757\n",
      "Epoch 1901/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 8659.0642 - val_loss: 11097.5946\n",
      "Epoch 1902/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10042.0259 - val_loss: 13535.2771\n",
      "Epoch 1903/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 12008.1962 - val_loss: 11909.0926\n",
      "Epoch 1904/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12858.3335 - val_loss: 9759.8541\n",
      "Epoch 1905/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 9022.0332 - val_loss: 10305.3354\n",
      "Epoch 1906/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12200.4565 - val_loss: 9385.7148\n",
      "Epoch 1907/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 9988.9926 - val_loss: 9147.1993\n",
      "Epoch 1908/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10157.1535 - val_loss: 10839.8406\n",
      "Epoch 1909/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 10019.2974 - val_loss: 9455.9167\n",
      "Epoch 1910/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 10988.0114 - val_loss: 11090.7453\n",
      "Epoch 1911/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 9688.2101 - val_loss: 10434.4328\n",
      "Epoch 1912/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11635.5238 - val_loss: 11271.1754\n",
      "Epoch 1913/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 12130.5195 - val_loss: 7280.7124\n",
      "Epoch 1914/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 7915.5374 - val_loss: 10699.5173\n",
      "Epoch 1915/2000\n",
      "2705/2705 [==============================] - 0s 6us/step - loss: 11647.9889 - val_loss: 7062.7755\n",
      "Epoch 1916/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 7145.7689 - val_loss: 12099.4388\n",
      "Epoch 1917/2000\n",
      "2705/2705 [==============================] - 0s 6us/step - loss: 11492.3355 - val_loss: 9325.9302\n",
      "Epoch 1918/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 9784.3820 - val_loss: 11765.0422\n",
      "Epoch 1919/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 10437.4944 - val_loss: 10148.4814\n",
      "Epoch 1920/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 9874.2769 - val_loss: 9613.1820\n",
      "Epoch 1921/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 8711.4285 - val_loss: 10247.5217\n",
      "Epoch 1922/2000\n",
      "2705/2705 [==============================] - 0s 6us/step - loss: 11467.9513 - val_loss: 8502.8021\n",
      "Epoch 1923/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 11240.8316 - val_loss: 5883.8937\n",
      "Epoch 1924/2000\n",
      "2705/2705 [==============================] - 0s 6us/step - loss: 9163.8850 - val_loss: 6805.0222\n",
      "Epoch 1925/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 8735.1336 - val_loss: 7588.4658\n",
      "Epoch 1926/2000\n",
      "2705/2705 [==============================] - 0s 6us/step - loss: 8958.7143 - val_loss: 9075.7164\n",
      "Epoch 1927/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 8554.3946 - val_loss: 10829.9307\n",
      "Epoch 1928/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 11787.9516 - val_loss: 8118.5906\n",
      "Epoch 1929/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 9140.1368 - val_loss: 8027.8460\n",
      "Epoch 1930/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 11010.0875 - val_loss: 7320.5371\n",
      "Epoch 1931/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 8406.7995 - val_loss: 7242.0960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1932/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 7367.9919 - val_loss: 12638.1636\n",
      "Epoch 1933/2000\n",
      "2705/2705 [==============================] - 0s 6us/step - loss: 11516.2732 - val_loss: 9212.2379\n",
      "Epoch 1934/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 10871.2700 - val_loss: 7511.3942\n",
      "Epoch 1935/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 7169.3764 - val_loss: 8090.6972\n",
      "Epoch 1936/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 8053.8964 - val_loss: 10927.9066\n",
      "Epoch 1937/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11914.7885 - val_loss: 5453.7113\n",
      "Epoch 1938/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 6901.7610 - val_loss: 6871.1714\n",
      "Epoch 1939/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 6867.6213 - val_loss: 8712.2504\n",
      "Epoch 1940/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 9443.4395 - val_loss: 8561.8655\n",
      "Epoch 1941/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 8764.4357 - val_loss: 8002.4651\n",
      "Epoch 1942/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 8319.8585 - val_loss: 8785.5720\n",
      "Epoch 1943/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 8433.5965 - val_loss: 8973.0944\n",
      "Epoch 1944/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 9301.8380 - val_loss: 12246.2641\n",
      "Epoch 1945/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 11548.5347 - val_loss: 7376.6448\n",
      "Epoch 1946/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 9403.0293 - val_loss: 7042.3246\n",
      "Epoch 1947/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 6979.3492 - val_loss: 7823.0604\n",
      "Epoch 1948/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 9806.7379 - val_loss: 8844.4184\n",
      "Epoch 1949/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 8144.5918 - val_loss: 9653.3792\n",
      "Epoch 1950/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11377.1619 - val_loss: 7959.2007\n",
      "Epoch 1951/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 8787.9378 - val_loss: 6489.1412\n",
      "Epoch 1952/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 7930.5788 - val_loss: 9168.7951\n",
      "Epoch 1953/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 9572.5986 - val_loss: 8050.0356\n",
      "Epoch 1954/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 10305.3318 - val_loss: 7744.7237\n",
      "Epoch 1955/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10824.4241 - val_loss: 4790.1829\n",
      "Epoch 1956/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 6620.9409 - val_loss: 7525.4590\n",
      "Epoch 1957/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 8186.4728 - val_loss: 7461.2520\n",
      "Epoch 1958/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 8727.6606 - val_loss: 7978.8389\n",
      "Epoch 1959/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 8944.9242 - val_loss: 8016.7063\n",
      "Epoch 1960/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 9466.8328 - val_loss: 8839.0228\n",
      "Epoch 1961/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 9859.9527 - val_loss: 7142.1097\n",
      "Epoch 1962/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 9056.2391 - val_loss: 7754.7693\n",
      "Epoch 1963/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 8553.3800 - val_loss: 8351.4879\n",
      "Epoch 1964/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 10174.0625 - val_loss: 9270.6920\n",
      "Epoch 1965/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 8907.8492 - val_loss: 9456.4483\n",
      "Epoch 1966/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 10273.0347 - val_loss: 8248.1929\n",
      "Epoch 1967/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 9652.7792 - val_loss: 8422.4258\n",
      "Epoch 1968/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 9855.9379 - val_loss: 10983.4837\n",
      "Epoch 1969/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11053.1900 - val_loss: 9725.7007\n",
      "Epoch 1970/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 9455.9102 - val_loss: 12219.1885\n",
      "Epoch 1971/2000\n",
      "2705/2705 [==============================] - ETA: 0s - loss: 13622.500 - 0s 9us/step - loss: 14012.6285 - val_loss: 6108.6854\n",
      "Epoch 1972/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 8206.1551 - val_loss: 7755.9815\n",
      "Epoch 1973/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 8608.7715 - val_loss: 8765.6904\n",
      "Epoch 1974/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10983.2829 - val_loss: 10529.0755\n",
      "Epoch 1975/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 12234.8107 - val_loss: 14781.8630\n",
      "Epoch 1976/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15693.6750 - val_loss: 17277.0818\n",
      "Epoch 1977/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 17583.8734 - val_loss: 14258.5428\n",
      "Epoch 1978/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 16118.9790 - val_loss: 16262.7006\n",
      "Epoch 1979/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 15219.8008 - val_loss: 15727.0991\n",
      "Epoch 1980/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 14757.6249 - val_loss: 17808.9050\n",
      "Epoch 1981/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 15884.8891 - val_loss: 14622.5802\n",
      "Epoch 1982/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 15133.4466 - val_loss: 14987.8210\n",
      "Epoch 1983/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 13912.1086 - val_loss: 11133.4016\n",
      "Epoch 1984/2000\n",
      "2705/2705 [==============================] - 0s 7us/step - loss: 13640.5814 - val_loss: 13258.4352\n",
      "Epoch 1985/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13192.1480 - val_loss: 11542.0522\n",
      "Epoch 1986/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13240.1962 - val_loss: 12519.3546\n",
      "Epoch 1987/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 11947.2418 - val_loss: 11867.2999\n",
      "Epoch 1988/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 13614.2029 - val_loss: 13228.5810\n",
      "Epoch 1989/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12628.2750 - val_loss: 12095.0323\n",
      "Epoch 1990/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 11673.6496 - val_loss: 15004.2437\n",
      "Epoch 1991/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 13536.4088 - val_loss: 12130.9521\n",
      "Epoch 1992/2000\n",
      "2705/2705 [==============================] - 0s 10us/step - loss: 14901.5013 - val_loss: 11989.7552\n",
      "Epoch 1993/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10864.1287 - val_loss: 13691.2564\n",
      "Epoch 1994/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 14752.6176 - val_loss: 12866.5569\n",
      "Epoch 1995/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12463.5058 - val_loss: 10693.4981\n",
      "Epoch 1996/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 12898.0641 - val_loss: 8945.6842\n",
      "Epoch 1997/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 9824.9527 - val_loss: 9028.3776\n",
      "Epoch 1998/2000\n",
      "2705/2705 [==============================] - 0s 9us/step - loss: 9830.6128 - val_loss: 12025.7896\n",
      "Epoch 1999/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 10367.2655 - val_loss: 11421.0834\n",
      "Epoch 2000/2000\n",
      "2705/2705 [==============================] - 0s 8us/step - loss: 14236.1197 - val_loss: 7681.4427\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f21c27a78e0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=2000,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "raw_centroid_df = raw_df[raw_df['game_start'] > datetime.datetime(2020, 1, 1)]\n",
    "centroid_df = raw_centroid_df[data_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rawmapper = umap.UMAP().fit(encoder.predict(centroid_df.values))\n",
    "#umap.plot.points(rawmapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "encoded_space = encoder.predict(centroid_df.values)\n",
    "kmeans = KMeans(n_clusters=7).fit(encoded_space)\n",
    "dbscan = DBSCAN(min_samples=10, eps=0.3).fit(encoded_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_prediction = kmeans.predict(encoded_space)\n",
    "dbscan_prediction = dbscan.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_embedding = umap.UMAP(random_state=42).fit_transform(encoded_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f2118333a30>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.scatter(standard_embedding[:, 0], standard_embedding[:, 1], c=kmeans_prediction, s=0.1, cmap='Spectral')\n",
    "plt.scatter(standard_embedding[:, 0], standard_embedding[:, 1], c=kmeans_prediction, s=0.1, cmap='Spectral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _distance(point_a, point_b):\n",
    "    delta = point_a - point_b\n",
    "    return np.sqrt(np.matmul(delta, delta.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_distance_from_cluster(row):\n",
    "    return min(_distance(cluster, row) for cluster in clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clusters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-b83622e578a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_distance_from_cluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mapply_along_axis\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/sc2creativity-xCKdH-6A-py3.8/lib/python3.8/site-packages/numpy/lib/shape_base.py\u001b[0m in \u001b[0;36mapply_along_axis\u001b[0;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot apply_along_axis when any iteration dimensions are 0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minarr_view\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;31m# build a buffer for storing evaluations of func1d.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-e06a72dc2efe>\u001b[0m in \u001b[0;36mmin_distance_from_cluster\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmin_distance_from_cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcluster\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'clusters' is not defined"
     ]
    }
   ],
   "source": [
    "distances = np.apply_along_axis(min_distance_from_cluster, 1, encoded_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_centroid_df['creativity'] = distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_centroid_df = raw_centroid_df.sort_values('creativity', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_centroid_df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_centroid_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_player(player_name):\n",
    "    d = raw_centroid_df[raw_centroid_df.self_name.str.lower() == player_name.lower()].creativity\n",
    "    print(d.describe())\n",
    "    return sns.distplot(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_player('showtime')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_player(\"has\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_player(\"maxpax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_player(\"stats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
